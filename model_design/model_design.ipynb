{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from importlib import import_module\n",
    "\n",
    "# Change working directory\n",
    "os.chdir('/home/tanzl/code/githubdemo/THOR_DDPM')\n",
    "from model_zoo.ddpm import DDPM\n",
    "from PIL import Image\n",
    "\n",
    "# Set input and output directories\n",
    "# input_dir = sys.argv[1]\n",
    "# output_dir = sys.argv[2]\n",
    "input_dir = '/home/tanzl/data/mood/brainMRI/toy'\n",
    "output_dir = '/home/tanzl/data/mood/brainMRI/output_toy'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load configuration and model\n",
    "config_path = '/home/tanzl/code/githubdemo/THOR_DDPM/projects/thor/configs/mood_brainMRI/mood_brainMRI.yaml'\n",
    "pt_path = '/home/tanzl/code/githubdemo/THOR_DDPM/weights/thor/mood_brainMRI/trained_90epoch/best_model.pt'\n",
    "noise_level = 350\n",
    "\n",
    "\n",
    "# Load config\n",
    "with open(config_path, 'r') as stream_file:\n",
    "    config_file = yaml.load(stream_file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "global_model = torch.load(pt_path)\n",
    "\n",
    "model = DDPM(**(config_file['model']['params']))\n",
    "model.load_state_dict(global_model['model_weights'], strict=False)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Process NIfTI files\n",
    "nifti_files = [f for f in os.listdir(input_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "h, w = 256, 256\n",
    "\n",
    "for nifti_file in nifti_files:\n",
    "    nifti_path = os.path.join(input_dir, nifti_file)\n",
    "    nifti = nib.load(nifti_path)\n",
    "    data_array = nifti.get_fdata()\n",
    "    affine_matrix = nifti.affine\n",
    "\n",
    "    anomaly_maps = []\n",
    "\n",
    "    for slice_idx in tqdm(range(data_array.shape[2])):  # Assuming the third dimension is the slice\n",
    "        img = data_array[:, :, slice_idx]\n",
    "        # print(np.max(img),np.min(img))\n",
    "        if np.sum(img) < 200:\n",
    "            anomaly_maps.append(np.zeros((h, w)))\n",
    "            continue\n",
    "        img = img*255\n",
    "        img = Image.fromarray(img).convert('L').resize((128, 128))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        img = img.rotate(90, expand=True)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "\n",
    "        anomaly_map, _, _ = model.get_anomaly(img_tensor, noise_level=noise_level)\n",
    "        print(np.max(anomaly_map),np.min(anomaly_map))\n",
    "        anomaly_map = Image.fromarray(anomaly_map[0][0]*255).resize((h, w))\n",
    "        plt.imshow(anomaly_map)\n",
    "        plt.show()\n",
    "        anomaly_map = anomaly_map.rotate(-90, expand=True)\n",
    "        anomaly_map = (np.array(anomaly_map)/255)\n",
    "        anomaly_map = np.clip(anomaly_map, 0, 1)\n",
    "        print(np.max(anomaly_map),np.min(anomaly_map))\n",
    "        plt.imshow(anomaly_map)\n",
    "        plt.show()\n",
    "        print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "        anomaly_maps.append(anomaly_map)\n",
    "\n",
    "    # Stack all anomaly maps into a 3D array\n",
    "    anomaly_map_3d = np.stack(anomaly_maps, axis=-1)\n",
    "\n",
    "    \n",
    "    # Save the anomaly map as a new NIfTI file\n",
    "    output_nifti = nib.Nifti1Image(anomaly_map_3d, affine=affine_matrix)\n",
    "    output_file = os.path.join(output_dir, f\"anomaly_{nifti_file}\")\n",
    "    nib.save(output_nifti, output_file)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anomaly_maps)):\n",
    "    # if anomaly_maps[i].shape[2] == 1:\n",
    "    anomaly_maps[i] = anomaly_maps[i].reshape(h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_map_3d = np.stack(anomaly_maps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the anomaly map as a new NIfTI file\n",
    "output_nifti = nib.Nifti1Image(anomaly_map_3d, affine=affine_matrix)\n",
    "output_file = os.path.join(output_dir, f\"anomaly_{nifti_file}\")\n",
    "nib.save(output_nifti, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_maps[128].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断是否存在异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不存在显著异常区域\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "def load_nifti(nifti_path):\n",
    "    nifti = nib.load(nifti_path)\n",
    "    data_array = nifti.get_fdata()\n",
    "    return data_array\n",
    "\n",
    "def detect_significant_anomaly(data_array, window_size=5, threshold_factor=2):\n",
    "    # 计算局部平均\n",
    "    local_mean = uniform_filter(data_array, size=window_size)\n",
    "\n",
    "    # 计算全局平均和标准差\n",
    "    global_mean = np.mean(data_array)\n",
    "    global_std = np.std(data_array)\n",
    "\n",
    "    # 定义显著程度的阈值\n",
    "    threshold = global_mean + threshold_factor * global_std\n",
    "\n",
    "    # 判断是否存在显著异常区域\n",
    "    significant_anomaly = np.any(local_mean > threshold)\n",
    "\n",
    "    return significant_anomaly\n",
    "\n",
    "# 示例用法\n",
    "window_size = 10\n",
    "threshold_factor = 4\n",
    "\n",
    "nifti_path = '/home/tanzl/data/mood/brainMRI/output_toy/anomaly_toy_0.nii.gz'\n",
    "data_array = load_nifti(nifti_path)\n",
    "exists_anomaly = detect_significant_anomaly(data_array=data_array,window_size=window_size, threshold_factor=threshold_factor)\n",
    "\n",
    "print(\"存在显著异常区域\" if exists_anomaly else \"不存在显著异常区域\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "# import wandb\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('/home/tanzl/code/githubdemo/THOR_DDPM')\n",
    "from dl_utils.config_utils import *\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "print(os.getcwd())  # 检查当前工作目录\n",
    "os.chdir('/home/tanzl/code/githubdemo/THOR_DDPM')\n",
    "print(os.getcwd())  # \n",
    "\n",
    "\n",
    "from net_utils.simplex_noise import generate_noise, generate_simplex_noise\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_image_as_tensor(image_path):\n",
    "    \"\"\"\n",
    "    读取图像并转换为形状为 (1, 1, 128, 128) 的 PyTorch 张量。\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = transform(image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "def display_images(img, rec):\n",
    "    \"\"\"\n",
    "    将 img, rec 和 np.abs(img - rec) 以一行三列的方式展示。\n",
    "\n",
    "    参数:\n",
    "    img (numpy.ndarray): 第一个图像数组，形状为 (1, 128, 128)\n",
    "    rec (numpy.ndarray): 第二个图像数组，形状为 (1, 128, 128)\n",
    "    \"\"\"\n",
    "    # 计算绝对差值图像\n",
    "    diff = np.abs(img - rec)\n",
    "\n",
    "    # 去除第一个维度以便于显示\n",
    "    img = img.squeeze()\n",
    "    rec = rec.squeeze()\n",
    "    diff = diff.squeeze()\n",
    "\n",
    "    # 创建一个图形和子图\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 显示 img\n",
    "    axes[0].imshow(img, cmap='gray')\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')  # 关闭坐标轴\n",
    "\n",
    "    # 显示 rec\n",
    "    axes[1].imshow(rec, cmap='gray')\n",
    "    axes[1].set_title('Reconstructed')\n",
    "    axes[1].axis('off')  # 关闭坐标轴\n",
    "\n",
    "    # 显示绝对差值图像\n",
    "    axes[2].imshow(diff, cmap='gray')\n",
    "    axes[2].set_title('Absolute Difference')\n",
    "    axes[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登录wandb\n",
    "import wandb\n",
    "wandb.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载config_file\n",
    "config_path = '/home/tanzl/code/githubdemo/THOR_DDPM/projects/thor/configs/mood_brainMRI/mood_brainMRI.yaml'\n",
    "# config_path = '/home/tanzl/code/githubdemo/THOR_DDPM/projects/thor/configs/brain/thor.yaml'\n",
    "# config_path = '/home/tanzl/code/githubdemo/THOR_DDPM/projects/thor/configs/mood_brainMRI/mood_brainMRI_augmented.yaml'\n",
    "stream_file = open(config_path, 'r')\n",
    "config_file = yaml.load(stream_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化DLConfigurator \n",
    "configurator_class = import_module(config_file['configurator']['module_name'],config_file['configurator']['class_name'])\n",
    "configurator = configurator_class(config_file=config_file, log_wandb=True) # core.Configurator.DLConfigurator\n",
    "exp_name = configurator.dl_config['experiment']['name'] # 'THOR'\n",
    "method_name = configurator.dl_config['name'] # 'THOR [Gaussian] AD 350'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLConfigurator.start_training(self, global_model: dict = dict())\n",
    "# 加载data\n",
    "data = configurator.load_data(configurator.dl_config['trainer']['data_loader'], train=True)\n",
    "val_ds = data.val_dataloader()\n",
    "train_ds = data.train_dataloader()\n",
    "\n",
    "# 加载traner_class然后实例化trainer\n",
    "trainer_class = import_module(configurator.dl_config['trainer']['module_name'], configurator.dl_config['trainer']['class_name']) # projects.thor.DDPMTrainer.PTrainer\n",
    "\n",
    "trainer = trainer_class(training_params=configurator.dl_config['trainer']['params'], model=copy.deepcopy(configurator.model),\n",
    "                                data=data, device=configurator.device, log_wandb=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 一些额外的信息\n",
    "# # DLConfigurator.load_data\n",
    "# data_loader_config = configurator.dl_config['trainer']['data_loader']\n",
    "# data_loader_module = import_module(data_loader_config['module_name'], data_loader_config['class_name'])\n",
    "\n",
    "\n",
    "\n",
    "# configurator.model # DDPM\n",
    "# configurator.dl_config['trainer']['params']\n",
    "# '''\n",
    "# {'input_size': (128, 128),\n",
    "#  'checkpoint_path': './weights/thor/thor/',\n",
    "#  'batch_size': 8,\n",
    "#  'nr_epochs': 1500,\n",
    "#  'patience': 1500,\n",
    "#  'val_interval': 1,\n",
    "#  'loss': {'module_name': 'optim.losses.ln_losses',\n",
    "#   'class_name': 'L2',\n",
    "#   'params': None},\n",
    "#  'optimizer_params': {'lr': 0.0001}}\n",
    "# '''\n",
    "# trainer.transform # None\n",
    "\n",
    "# configurator.dl_config['downstream_tasks']['detection']['checkpoint_path'] # './weights/thor/mood_brainMRI/'\n",
    "\n",
    "# global_model.keys() # dict_keys(['model_weights', 'optimizer_weights', 'epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "device = configurator.device\n",
    "pt_path = '/home/tanzl/code/githubdemo/THOR_DDPM/weights/thor/mood_brainMRI/trained_90epoch/best_model.pt'\n",
    "global_model = torch.load(pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行下游任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurator.start_evaluations(global_model['model_weights'], 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_level in tqdm(range(0,1000,50)):\n",
    "    configurator.start_evaluations(global_model['model_weights'], noise_level)\n",
    "# configurator.start_evaluations(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurator.dl_config['downstream_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化下游任务类\n",
    "# 定义下游任务的参数\n",
    "dst_name = 'detection'\n",
    "dst_config = configurator.dl_config['downstream_tasks'][dst_name]\n",
    "downstream_class = import_module(dst_config['module_name'], dst_config['class_name'])\n",
    "data = configurator.load_data(dst_config['data_loader'], train=False)\n",
    "# 下游任务类\n",
    "dst = downstream_class(dst_name, configurator.model, configurator.device, data, dst_config['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst.start_task(global_model=global_model,noise_level=noise_level)\n",
    "global_model = global_model\n",
    "noise_level = 350\n",
    "dst.pathology_localization(global_model, 3, 71, True, noise_level=noise_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# def detect_anomalies(self, global_model, path, noise_level=350, batch_size=1):\n",
    "#     \"\"\"\n",
    "#     执行异常检测并获取异常分数\n",
    "#     :param global_model: dict, 模型参数\n",
    "#     :param path: str, 图像或目录路径\n",
    "#     :param noise_level: int, 噪声水平\n",
    "#     :param batch_size: int, 批次大小\n",
    "#     :return: x, x_rec, anomaly_score, anomaly_map\n",
    "#     \"\"\"\n",
    "self = dst\n",
    "path = '/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/toy/'\n",
    "noise_level=350\n",
    "batch_size=32\n",
    "\n",
    "self.model.load_state_dict(global_model['model_weights'], strict=False)\n",
    "self.model.eval()\n",
    "\n",
    "# 判断路径是文件还是目录\n",
    "if os.path.isfile(path):\n",
    "    img = Image.open(path).convert('L').resize((128,128))\n",
    "    img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "    x = img_tensor\n",
    "    anomaly_map, anomaly_score, x_rec_dict = self.model.get_anomaly(copy.deepcopy(x), noise_level=noise_level)\n",
    "    x_rec = x_rec_dict['x_rec'] if 'x_rec' in x_rec_dict.keys() else torch.zeros_like(x)\n",
    "    x_rec = torch.clamp(x_rec, 0, 1)\n",
    "    # return x, x_rec, anomaly_score, anomaly_map, x_rec_dict\n",
    "\n",
    "elif os.path.isdir(path):\n",
    "    img_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.png')]\n",
    "    all_x, all_x_rec, all_anomaly_scores, all_anomaly_maps = [], [], [], []\n",
    "\n",
    "    # 分批次进行预测\n",
    "    for i in tqdm(range(0, len(img_files), batch_size)):\n",
    "        batch_files = img_files[i:i+batch_size]\n",
    "        imgs = [transforms.ToTensor()(Image.open(f).convert('L').resize((128,128))) for f in batch_files]\n",
    "        img_tensor = torch.stack(imgs).to(self.device)\n",
    "\n",
    "        print(img_tensor.shape)\n",
    "        x = img_tensor\n",
    "        anomaly_map, anomaly_score, x_rec_dict = self.model.get_anomaly(copy.deepcopy(x), noise_level=noise_level)\n",
    "        x_rec = x_rec_dict['x_rec'] if 'x_rec' in x_rec_dict.keys() else torch.zeros_like(x)\n",
    "        x_rec = torch.clamp(x_rec, 0, 1)\n",
    "\n",
    "        all_x.append(x)\n",
    "        all_x_rec.append(x_rec)\n",
    "        all_anomaly_scores.append(anomaly_score)\n",
    "        all_anomaly_maps.append(anomaly_map)\n",
    "\n",
    "    # 拼接结果\n",
    "    all_x = torch.cat(all_x, dim=0)\n",
    "    all_x_rec = torch.cat(all_x_rec, dim=0)\n",
    "    all_anomaly_scores = torch.tensor(np.concatenate(all_anomaly_scores))\n",
    "    all_anomaly_maps = torch.tensor(np.concatenate(all_anomaly_maps))\n",
    "    ano_scores = [torch.max(a).item() for a in all_anomaly_maps]\n",
    "\n",
    "    # return all_x, all_x_rec, all_anomaly_scores, all_anomaly_maps\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"提供的路径无效\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_maps = torch.cat(all_anomaly_maps, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_maps = torch.tensor(np.concatenate(all_anomaly_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_scores[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in tqdm(enumerate(data['Atlas'])):\n",
    "    img_tensor = x\n",
    "    masks = x[1].to(device)\n",
    "    masks[masks > 0] = 1\n",
    "    if torch.sum(masks) > 0:\n",
    "        x = img_tensor[0].to(device)\n",
    "        anomaly_map, anomaly_score, x_rec_dict = self.model.get_anomaly(copy.deepcopy(x), noise_level=noise_level)\n",
    "        x_rec = x_rec_dict['x_rec'] if 'x_rec' in x_rec_dict.keys() else torch.zeros_like(x)\n",
    "        x_rec = torch.clamp(x_rec, 0, 1)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_np = x.cpu().detach().numpy().squeeze()\n",
    "x_rec_np = x_rec.cpu().detach().numpy().squeeze()\n",
    "anomaly_map_np = anomaly_map.squeeze()\n",
    "\n",
    "# 创建子图\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 显示输入图像\n",
    "axes[0].imshow(x_np, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 显示重建图像\n",
    "axes[1].imshow(x_rec_np, cmap='gray')\n",
    "axes[1].set_title('Reconstructed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 显示异常图\n",
    "im = axes[2].imshow(anomaly_map_np, cmap='plasma', vmin=0, vmax=1)\n",
    "axes[2].set_title('Anomaly Map')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# 添加颜色条\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "\n",
    "# 显示图像\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_np = x.cpu().detach().numpy().squeeze()\n",
    "x_rec_np = x_rec.cpu().detach().numpy().squeeze()\n",
    "anomaly_map_np = anomaly_map.squeeze()\n",
    "\n",
    "# 创建子图\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 显示输入图像\n",
    "axes[0].imshow(x_np, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 显示重建图像\n",
    "axes[1].imshow(x_rec_np, cmap='gray')\n",
    "axes[1].set_title('Reconstructed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 显示异常图\n",
    "im = axes[2].imshow(anomaly_map_np, cmap='plasma', vmin=0, vmax=1)\n",
    "axes[2].set_title('Anomaly Map')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# 添加颜色条\n",
    "fig.colorbar(im, ax=axes[2])\n",
    "\n",
    "# 显示图像\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = trainer.model # 也可以直接等于configurator.model\n",
    "test_model.load_state_dict(global_model['model_weights'])\n",
    "test_model.eval()\n",
    "test_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接读取图片进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '/home/tanzl/code/githubdemo/THOR_DDPM/'\n",
    "img_name = './data/brainMRI/png/val/toy_2_slice_107.png'\n",
    "img_path = img_root + img_name[1:]\n",
    "\n",
    "task = 'mood_brainMRI'\n",
    "metrics = {\n",
    "    task + '_loss_rec': 0,\n",
    "    task + '_loss_mse': 0,\n",
    "    task + '_loss_pl': 0,\n",
    "}\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = load_image_as_tensor(img_path)\n",
    "\n",
    "    b, c, h, w = x.shape\n",
    "    test_total += b\n",
    "    x = x.to(device)\n",
    "    x = (x * 2) - 1\n",
    "    x_, _ = test_model.sample_from_image(x, noise_level=trainer.model.noise_level_recon)\n",
    "    x = (x + 1) /2 \n",
    "    x_ = (x_ + 1) / 2\n",
    "    loss_rec = trainer.criterion_rec(x_, x)\n",
    "    loss_mse = trainer.criterion_MSE(x_, x)\n",
    "    loss_pl = trainer.criterion_PL(x_, x)\n",
    "\n",
    "    metrics[task + '_loss_rec'] += loss_rec.item() * x.size(0)\n",
    "    metrics[task + '_loss_mse'] += loss_mse.item() * x.size(0)\n",
    "    metrics[task + '_loss_pl'] += loss_pl.item() * x.size(0)\n",
    "\n",
    "    # 这里需要通过data加载出图片名称，找出对应的label图片，然后进行指标的计算。\n",
    "\n",
    "\n",
    "\n",
    "# 单独拿出来一个进行可视化\n",
    "rec = x_.detach().cpu()[0].numpy()\n",
    "img = x.detach().cpu()[0].numpy()\n",
    "\n",
    "\n",
    "for metric_key in metrics.keys():\n",
    "    metric_name = task + '/' + str(metric_key)\n",
    "    metric_score = metrics[metric_key] / test_total\n",
    "    print({metric_name: metric_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "display_images(img, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "data = train_ds.dataset.__getitem__(0)\n",
    "augment = data[0].detach().cpu()[0].numpy()\n",
    "origin = data[1].detach().cpu()[0].numpy()\n",
    "# plt.imshow(augment)\n",
    "# plt.show()\n",
    "# plt.imshow(origin)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "img_path= '/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train/00704_slice_075.png'\n",
    "img = cv2.imread(img_path)\n",
    "aug_path= '/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train_augmented/00704_slice_075_augmented.png'\n",
    "aug = cv2.imread(aug_path)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(aug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nii2png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def nii_to_png(nii_path, output_dir):\n",
    "    \"\"\"\n",
    "    将 NIfTI 文件中的每个切片保存为 PNG 图像。\n",
    "    :param nii_path: 输入的 NIfTI 文件路径\n",
    "    :param output_dir: 输出的 PNG 图像保存目录\n",
    "    \"\"\"\n",
    "    # 加载 NIfTI 文件\n",
    "    img = nib.load(nii_path)\n",
    "    img_data = img.get_fdata()\n",
    "\n",
    "    # 获取文件名（不包括扩展名）\n",
    "    base_name = os.path.basename(nii_path).split('.')[0]\n",
    "\n",
    "    # 创建输出目录（如果不存在）\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(img_data.shape)\n",
    "    # 遍历每个切片\n",
    "    for i in range(img_data.shape[2]):\n",
    "        \n",
    "        slice_data = img_data[:, :, i]\n",
    "        \n",
    "        # 归一化切片数据到 0-255\n",
    "        slice_norm = 255 * (slice_data - np.min(slice_data)) / (np.max(slice_data) - np.min(slice_data))\n",
    "        slice_uint8 = slice_norm.astype(np.uint8)\n",
    "        \n",
    "        # 创建 PIL 图像对象\n",
    "        img_pil = Image.fromarray(slice_uint8)\n",
    "        \n",
    "        # 保存 PNG 图像\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_slice_{i:03d}.png\")\n",
    "        img_pil.save(output_path)\n",
    "\n",
    "def batch_process_nii_files(nii_files, output_dir):\n",
    "    \"\"\"\n",
    "    批量处理多个 NIfTI 文件，将它们转换为 PNG 图像。\n",
    "    :param nii_files: NIfTI 文件路径的列表\n",
    "    :param output_dir: 输出的 PNG 图像保存目录\n",
    "    \"\"\"\n",
    "    for nii_path in tqdm(nii_files):\n",
    "        nii_to_png(nii_path, output_dir)\n",
    "\n",
    "# 示例用法\n",
    "nii_root = '/home/tanzl/data/mood/brainMRI/output_toy/'\n",
    "# nii_files = [os.path.join(nii_root,name) for name in os.listdir(nii_root)]\n",
    "nii_files = ['/home/tanzl/data/mood/brainMRI/output_toy/anomaly_toy_2.nii.gz']\n",
    "output_dir = '/home/tanzl/data/mood/brainMRI/output_toy/png'\n",
    "batch_process_nii_files(nii_files, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从一个文件夹到另一个文件夹转移图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_specific_images(src_dir, dest_dir, start_num, end_num):\n",
    "    \"\"\"\n",
    "    提取文件名尾部编号在指定范围内的图片文件到新目录。\n",
    "    \n",
    "    :param src_dir: 源图片目录路径\n",
    "    :param dest_dir: 目标图片目录路径\n",
    "    :param start_num: 编号范围的起始值（含）\n",
    "    :param end_num: 编号范围的结束值（含）\n",
    "    \"\"\"\n",
    "    # 创建目标目录（如果不存在）\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    # 遍历源目录中的所有文件\n",
    "    for filename in os.listdir(src_dir):\n",
    "        # 检查文件是否以.png结尾\n",
    "        if filename.endswith('.png'):\n",
    "            # 提取尾部编号\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            tail_num_str = base_name.split('_')[-1]\n",
    "            \n",
    "            if tail_num_str.isdigit():\n",
    "                tail_num = int(tail_num_str)\n",
    "                \n",
    "                # 检查编号是否在指定范围内\n",
    "                if start_num <= tail_num <= end_num:\n",
    "                    # 构建源文件路径和目标文件路径\n",
    "                    src_path = os.path.join(src_dir, filename)\n",
    "                    dest_path = os.path.join(dest_dir, filename)\n",
    "                    \n",
    "                    # 复制文件到目标目录\n",
    "                    shutil.copy(src_path, dest_path)\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "src_dir = '/home/tanzl/data/mood/ATLAS_val_png'\n",
    "dest_dir = '/home/tanzl/data/mood/ATLAS_val_png_selected'\n",
    "start_num = 90\n",
    "end_num = 95\n",
    "\n",
    "extract_specific_images(src_dir, dest_dir, start_num, end_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 旋转一个文件夹中的图片并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def rotate_images_in_directory(directory, angle=90):\n",
    "    \"\"\"\n",
    "    将指定目录中的所有 PNG 图片顺时针旋转指定角度。\n",
    "\n",
    "    :param directory: 图片所在目录路径\n",
    "    :param angle: 旋转角度，默认为90度（顺时针）\n",
    "    \"\"\"\n",
    "    # 确保目录存在\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"目录 {directory} 不存在\")\n",
    "        return\n",
    "\n",
    "    # 遍历目录中的所有文件\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        # 检查文件是否以.png结尾\n",
    "        if filename.endswith('.png'):\n",
    "            # 构建文件路径\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # 打开图片\n",
    "            with Image.open(file_path) as img:\n",
    "                # 旋转图片\n",
    "                rotated_img = img.rotate(-angle, expand=True)\n",
    "                \n",
    "                # 保存旋转后的图片，覆盖原文件\n",
    "                rotated_img.save(file_path)\n",
    "\n",
    "# 示例用法\n",
    "directory = '/home/tanzl/code/githubdemo/THOR_DDPM/data/abtomMRI/png/toy'\n",
    "rotate_images_in_directory(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取并保存csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# 定义包含 PNG 文件的文件夹路径\n",
    "folder_path = '/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/png/train_augmented'\n",
    "\n",
    "# 获取文件夹中的所有 PNG 文件名\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "\n",
    "# 完整的文件路径\n",
    "file_paths = [os.path.join(folder_path, f) for f in file_names]\n",
    "\n",
    "# 定义 CSV 文件的输出路径\n",
    "output_csv = '/home/tanzl/code/githubdemo/THOR_DDPM/data/brainMRI/splits/mood_brainMRI_train_augmented.csv'\n",
    "\n",
    "# 将文件名写入 CSV 文件\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['filename'])  # 写入表头\n",
    "    for file_path in file_paths:\n",
    "        writer.writerow([file_path])\n",
    "\n",
    "print(f'File names have been written to {output_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
