INFO:root:------------------------------- DEEP LEARNING FRAMEWORK *IML-COMPAI-DL*  -------------------------------
INFO:root:[IML-COMPAI-DL::main] Success: Loaded configuration file at: ./projects/thor/configs/brain/thor.yaml
INFO:root:[Main::setup_experiment]: ################ Starting setup ################
WARNING:root:WARNING: /home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/xformers/_C.so: undefined symbol: _ZN2at23shouldRunRecordFunctionEPb
Need to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop
WARNING:root:Triton is not available, some optimizations will not be enabled.
Error No module named 'triton'
INFO:root:[Main::setup_experiment]: ################ Starting experiment * THOR * using method * THOR [Gaussian] AD 350 * ################
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp6050d6lh
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp6050d6lh/_remote_module_non_scriptable.py
INFO:root:DefaultDataset::init(): Loading 581 files from: ['./data/CAPS_IXI/splits/ixi-t1_atlas_train_2D.csv']
INFO:root:DefaultDataset::init(): Loading 581 files from: ['./data/CAPS_IXI/splits/ixi-t1_atlas_train_2D.csv']
INFO:wandb:Watching
INFO:root:[Configurator::train]: ################ Starting training ################
/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/xformers/_C.so: undefined symbol: _ZN2at23shouldRunRecordFunctionEPb
Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [on]
Loading model from: /home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/lpips/weights/v0.1/squeeze.pth
****** DIFFUSION: Using DDPM Scheduler ******
****** DIFFUSION: Using DDPM Scheduler ******
BrainLoader
INFO: Early stopping delta 1e-08
Input size of summery is: (1, 1, 128, 128)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
DDPM                                               [1, 1, 128, 128]          724,736
├─DiffusionModelUNet: 1-1                          [1, 1, 128, 128]          --
│    └─Sequential: 2-1                             [1, 512]                  --
│    │    └─Linear: 3-1                            [1, 512]                  66,048
│    │    └─SiLU: 3-2                              [1, 512]                  --
│    │    └─Linear: 3-3                            [1, 512]                  262,656
│    └─Convolution: 2-2                            [1, 128, 128, 128]        --
│    │    └─Conv2d: 3-4                            [1, 128, 128, 128]        1,280
│    └─ModuleList: 2-3                             --                        --
│    │    └─DownBlock: 3-5                         [1, 128, 64, 64]          508,928
│    │    └─AttnDownBlock: 3-6                     [1, 256, 32, 32]          1,904,128
│    │    └─AttnDownBlock: 3-7                     [1, 256, 32, 32]          1,576,192
│    └─AttnMidBlock: 2-4                           [1, 256, 32, 32]          --
│    │    └─ResnetBlock: 3-8                       [1, 256, 32, 32]          1,312,512
│    │    └─AttentionBlock: 3-9                    [1, 256, 32, 32]          263,680
│    │    └─ResnetBlock: 3-10                      [1, 256, 32, 32]          1,312,512
│    └─ModuleList: 2-5                             --                        --
│    │    └─AttnUpBlock: 3-11                      [1, 256, 64, 64]          5,185,792
│    │    └─AttnUpBlock: 3-12                      [1, 256, 128, 128]        4,857,856
│    │    └─UpBlock: 3-13                          [1, 128, 128, 128]        1,248,000
│    └─Sequential: 2-6                             [1, 1, 128, 128]          --
│    │    └─GroupNorm: 3-14                        [1, 128, 128, 128]        256
│    │    └─SiLU: 3-15                             [1, 128, 128, 128]        --
│    │    └─Convolution: 3-16                      [1, 1, 128, 128]          1,153
====================================================================================================
Total params: 19,225,729
Trainable params: 18,503,233
Non-trainable params: 722,496
Total mult-adds (G): 61.95
====================================================================================================
Input size (MB): 0.07
Forward/backward pass size (MB): 685.93
Params size (MB): 72.16
Estimated Total Size (MB): 758.15
====================================================================================================
Epoch: 0 	Training Loss: 0.654272 , computed in 50.547884464263916 seconds for 576 samples
Epoch: 1 	Training Loss: 0.137673 , computed in 50.24758172035217 seconds for 576 samples
Epoch: 2 	Training Loss: 0.051951 , computed in 50.534748792648315 seconds for 576 samples
Epoch: 3 	Training Loss: 0.041731 , computed in 50.88951635360718 seconds for 576 samples
INFO: Early stopping counter 1 of 1500 with -0.010822713375091553
Epoch: 4 	Training Loss: 0.036027 , computed in 50.8479163646698 seconds for 576 samples
INFO: Early stopping counter 2 of 1500 with -0.01768970489501953
Epoch: 5 	Training Loss: 0.035822 , computed in 50.46047329902649 seconds for 576 samples
INFO: Early stopping counter 3 of 1500 with -0.02049320936203003
Epoch: 6 	Training Loss: 0.036373 , computed in 50.286072731018066 seconds for 576 samples
INFO: Early stopping counter 4 of 1500 with -0.01816415786743164
