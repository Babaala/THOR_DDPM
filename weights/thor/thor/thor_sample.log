nohup: ignoring input
INFO:root:------------------------------- DEEP LEARNING FRAMEWORK *IML-COMPAI-DL*  -------------------------------
INFO:root:[IML-COMPAI-DL::main] Success: Loaded configuration file at: ./projects/thor/configs/brain/thor.yaml
INFO:root:[Main::setup_experiment]: ################ Starting setup ################
WARNING:root:WARNING: /home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/xformers/_C.so: undefined symbol: _ZN2at23shouldRunRecordFunctionEPb
Need to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop
WARNING:root:Blocksparse is not available: the current GPU does not expose Tensor cores
INFO:root:[Main::setup_experiment]: ################ Starting experiment * THOR * using method * THOR [Gaussian] AD 350 * ################
wandb: Currently logged in as: 1078297362 (spongestark). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/tanzl/code/githubdemo/THOR_DDPM/wandb/run-20240728_192120-2024_07_28_19_21_12_133071
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run THOR [Gaussian] AD 350
wandb: ‚≠êÔ∏è View project at https://wandb.ai/spongestark/THOR
wandb: üöÄ View run at https://wandb.ai/spongestark/THOR/runs/2024_07_28_19_21_12_133071
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpn51bf0kc
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpn51bf0kc/_remote_module_non_scriptable.py
INFO:root:DefaultDataset::init(): Loading 529 files from: ['./data/CAPS_IXI/splits/ixi-t1_atlas_train_2D.csv']
INFO:root:DefaultDataset::init(): Loading 52 files from: ['./data/CAPS_IXI/splits/ixi-t1_atlas_val_2D.csv']
INFO:root:[Configurator::train]: ################ Starting training ################
/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/xformers/_C.so: undefined symbol: _ZN2at23shouldRunRecordFunctionEPb
Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [on]
Loading model from: /home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/lpips/weights/v0.1/squeeze.pth
****** DIFFUSION: Using DDPM Scheduler ******
****** DIFFUSION: Using DDPM Scheduler ******
BrainLoader
INFO: Early stopping delta 1e-08
Input size of summery is: (1, 1, 128, 128)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
DDPM                                               [1, 1, 128, 128]          724,736
‚îú‚îÄDiffusionModelUNet: 1-1                          [1, 1, 128, 128]          --
‚îÇ    ‚îî‚îÄSequential: 2-1                             [1, 512]                  --
‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-1                            [1, 512]                  66,048
‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-2                              [1, 512]                  --
‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-3                            [1, 512]                  262,656
‚îÇ    ‚îî‚îÄConvolution: 2-2                            [1, 128, 128, 128]        --
‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                            [1, 128, 128, 128]        1,280
‚îÇ    ‚îî‚îÄModuleList: 2-3                             --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄDownBlock: 3-5                         [1, 128, 64, 64]          508,928
‚îÇ    ‚îÇ    ‚îî‚îÄAttnDownBlock: 3-6                     [1, 256, 32, 32]          1,904,128
‚îÇ    ‚îÇ    ‚îî‚îÄAttnDownBlock: 3-7                     [1, 256, 32, 32]          1,576,192
‚îÇ    ‚îî‚îÄAttnMidBlock: 2-4                           [1, 256, 32, 32]          --
‚îÇ    ‚îÇ    ‚îî‚îÄResnetBlock: 3-8                       [1, 256, 32, 32]          1,312,512
‚îÇ    ‚îÇ    ‚îî‚îÄAttentionBlock: 3-9                    [1, 256, 32, 32]          263,680
‚îÇ    ‚îÇ    ‚îî‚îÄResnetBlock: 3-10                      [1, 256, 32, 32]          1,312,512
‚îÇ    ‚îî‚îÄModuleList: 2-5                             --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄAttnUpBlock: 3-11                      [1, 256, 64, 64]          5,185,792
‚îÇ    ‚îÇ    ‚îî‚îÄAttnUpBlock: 3-12                      [1, 256, 128, 128]        4,857,856
‚îÇ    ‚îÇ    ‚îî‚îÄUpBlock: 3-13                          [1, 128, 128, 128]        1,248,000
‚îÇ    ‚îî‚îÄSequential: 2-6                             [1, 1, 128, 128]          --
‚îÇ    ‚îÇ    ‚îî‚îÄGroupNorm: 3-14                        [1, 128, 128, 128]        256
‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-15                             [1, 128, 128, 128]        --
‚îÇ    ‚îÇ    ‚îî‚îÄConvolution: 3-16                      [1, 1, 128, 128]          1,153
====================================================================================================
Total params: 19,225,729
Trainable params: 18,503,233
Non-trainable params: 722,496
Total mult-adds (G): 61.95
====================================================================================================
Input size (MB): 0.07
Forward/backward pass size (MB): 685.93
Params size (MB): 72.16
Estimated Total Size (MB): 758.15
====================================================================================================
Epoch: 0 	Training Loss: 0.068008 , computed in 45.274256229400635 seconds for 528 samples
Epoch: 1 	Training Loss: 0.036179 , computed in 44.38007736206055 seconds for 528 samples
Epoch: 2 	Training Loss: 0.033105 , computed in 44.88340950012207 seconds for 528 samples
Epoch: 3 	Training Loss: 0.034069 , computed in 44.67500281333923 seconds for 528 samples
Epoch: 4 	Training Loss: 0.034051 , computed in 45.007651567459106 seconds for 528 samples
Epoch: 5 	Training Loss: 0.032823 , computed in 45.292190074920654 seconds for 528 samples
Epoch: 6 	Training Loss: 0.031105 , computed in 44.737114667892456 seconds for 528 samples
Epoch: 7 	Training Loss: 0.033074 , computed in 45.1874258518219 seconds for 528 samples
Epoch: 8 	Training Loss: 0.031701 , computed in 44.86794090270996 seconds for 528 samples
Epoch: 9 	Training Loss: 0.030664 , computed in 44.862943172454834 seconds for 528 samples
Epoch: 10 	Training Loss: 0.030276 , computed in 45.61305785179138 seconds for 528 samples
Epoch: 11 	Training Loss: 0.029485 , computed in 45.22698903083801 seconds for 528 samples
Epoch: 12 	Training Loss: 0.030656 , computed in 44.91367483139038 seconds for 528 samples
Epoch: 13 	Training Loss: 0.029987 , computed in 44.83446717262268 seconds for 528 samples
Epoch: 14 	Training Loss: 0.029344 , computed in 45.297364950180054 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0013002939522266388
Epoch: 15 	Training Loss: 0.032461 , computed in 45.01344180107117 seconds for 528 samples
Epoch: 16 	Training Loss: 0.029531 , computed in 44.78616213798523 seconds for 528 samples
Epoch: 17 	Training Loss: 0.029400 , computed in 45.31953430175781 seconds for 528 samples
Epoch: 18 	Training Loss: 0.030370 , computed in 45.0180447101593 seconds for 528 samples
Epoch: 19 	Training Loss: 0.028597 , computed in 45.3753342628479 seconds for 528 samples
Epoch: 20 	Training Loss: 0.028460 , computed in 44.97546982765198 seconds for 528 samples
Epoch: 21 	Training Loss: 0.029796 , computed in 44.81013035774231 seconds for 528 samples
Epoch: 22 	Training Loss: 0.028155 , computed in 45.33499622344971 seconds for 528 samples
Epoch: 23 	Training Loss: 0.031381 , computed in 44.94212222099304 seconds for 528 samples
Epoch: 24 	Training Loss: 0.028391 , computed in 44.810516595840454 seconds for 528 samples
Epoch: 25 	Training Loss: 0.027838 , computed in 45.22748160362244 seconds for 528 samples
Epoch: 26 	Training Loss: 0.029037 , computed in 45.02112627029419 seconds for 528 samples
Epoch: 27 	Training Loss: 0.029459 , computed in 45.449936389923096 seconds for 528 samples
Epoch: 28 	Training Loss: 0.030560 , computed in 45.220561265945435 seconds for 528 samples
Epoch: 29 	Training Loss: 0.028105 , computed in 45.15047264099121 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.05085466802120209
Epoch: 30 	Training Loss: 0.028711 , computed in 45.11824584007263 seconds for 528 samples
Epoch: 31 	Training Loss: 0.027912 , computed in 45.2570743560791 seconds for 528 samples
Epoch: 32 	Training Loss: 0.028484 , computed in 44.9765944480896 seconds for 528 samples
Epoch: 33 	Training Loss: 0.029727 , computed in 45.095322370529175 seconds for 528 samples
Epoch: 34 	Training Loss: 0.028930 , computed in 45.45327425003052 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.015127278864383698
Epoch: 35 	Training Loss: 0.027442 , computed in 44.981281042099 seconds for 528 samples
Epoch: 36 	Training Loss: 0.028055 , computed in 45.3613121509552 seconds for 528 samples
Epoch: 37 	Training Loss: 0.027691 , computed in 44.753819942474365 seconds for 528 samples
Epoch: 38 	Training Loss: 0.026739 , computed in 44.90817952156067 seconds for 528 samples
Epoch: 39 	Training Loss: 0.028846 , computed in 44.79996204376221 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.0003588348627090454
Epoch: 40 	Training Loss: 0.028089 , computed in 45.36103940010071 seconds for 528 samples
Epoch: 41 	Training Loss: 0.025327 , computed in 45.15615510940552 seconds for 528 samples
Epoch: 42 	Training Loss: 0.027086 , computed in 45.17195653915405 seconds for 528 samples
Epoch: 43 	Training Loss: 0.025807 , computed in 45.16718244552612 seconds for 528 samples
Epoch: 44 	Training Loss: 0.027960 , computed in 45.138253927230835 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.007713142782449722
Epoch: 45 	Training Loss: 0.027581 , computed in 45.46569871902466 seconds for 528 samples
Epoch: 46 	Training Loss: 0.026998 , computed in 45.016244888305664 seconds for 528 samples
Epoch: 47 	Training Loss: 0.028117 , computed in 44.57300591468811 seconds for 528 samples
Epoch: 48 	Training Loss: 0.027340 , computed in 44.853973388671875 seconds for 528 samples
Epoch: 49 	Training Loss: 0.024966 , computed in 45.0931510925293 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.00853785127401352
Epoch: 50 	Training Loss: 0.025460 , computed in 44.98477911949158 seconds for 528 samples
wandb: Network error (ConnectionError), entering retry loop.
Epoch: 51 	Training Loss: 0.026504 , computed in 44.63432717323303 seconds for 528 samples
Epoch: 52 	Training Loss: 0.027642 , computed in 44.98290705680847 seconds for 528 samples
Epoch: 53 	Training Loss: 0.026079 , computed in 44.98891472816467 seconds for 528 samples
Epoch: 54 	Training Loss: 0.027344 , computed in 45.17938590049744 seconds for 528 samples
Epoch: 55 	Training Loss: 0.025692 , computed in 45.3614706993103 seconds for 528 samples
wandb: Network error (ConnectionError), entering retry loop.
Epoch: 56 	Training Loss: 0.026528 , computed in 44.99480223655701 seconds for 528 samples
Epoch: 57 	Training Loss: 0.025169 , computed in 45.208179235458374 seconds for 528 samples
Epoch: 58 	Training Loss: 0.025519 , computed in 45.147799491882324 seconds for 528 samples
Epoch: 59 	Training Loss: 0.025838 , computed in 44.97751212120056 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.013777090236544609
Epoch: 60 	Training Loss: 0.027093 , computed in 45.4619243144989 seconds for 528 samples
Epoch: 61 	Training Loss: 0.025836 , computed in 44.78128671646118 seconds for 528 samples
Epoch: 62 	Training Loss: 0.024735 , computed in 44.617374897003174 seconds for 528 samples
Epoch: 63 	Training Loss: 0.025671 , computed in 45.02387452125549 seconds for 528 samples
Epoch: 64 	Training Loss: 0.026783 , computed in 45.69345188140869 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.002278946340084076
Epoch: 65 	Training Loss: 0.027030 , computed in 44.87723135948181 seconds for 528 samples
Epoch: 66 	Training Loss: 0.024305 , computed in 44.62176513671875 seconds for 528 samples
Epoch: 67 	Training Loss: 0.025634 , computed in 44.891690254211426 seconds for 528 samples
Epoch: 68 	Training Loss: 0.024532 , computed in 45.561607122421265 seconds for 528 samples
Epoch: 69 	Training Loss: 0.025591 , computed in 45.03616261482239 seconds for 528 samples
Epoch: 70 	Training Loss: 0.026573 , computed in 45.739323139190674 seconds for 528 samples
Epoch: 71 	Training Loss: 0.026135 , computed in 45.224438428878784 seconds for 528 samples
Epoch: 72 	Training Loss: 0.026910 , computed in 44.6529643535614 seconds for 528 samples
Epoch: 73 	Training Loss: 0.025668 , computed in 45.30515480041504 seconds for 528 samples
Epoch: 74 	Training Loss: 0.026567 , computed in 45.111226081848145 seconds for 528 samples
wandb: Network error (ConnectionError), entering retry loop.
wandb: Network error (ConnectionError), entering retry loop.
Epoch: 75 	Training Loss: 0.026404 , computed in 45.11223030090332 seconds for 528 samples
Epoch: 76 	Training Loss: 0.024943 , computed in 44.98292851448059 seconds for 528 samples
Epoch: 77 	Training Loss: 0.024601 , computed in 45.40366554260254 seconds for 528 samples
Epoch: 78 	Training Loss: 0.024975 , computed in 45.41111135482788 seconds for 528 samples
Epoch: 79 	Training Loss: 0.026758 , computed in 45.48708653450012 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.001977691426873207
Epoch: 80 	Training Loss: 0.026223 , computed in 44.93756985664368 seconds for 528 samples
Epoch: 81 	Training Loss: 0.026804 , computed in 45.488871812820435 seconds for 528 samples
Epoch: 82 	Training Loss: 0.026252 , computed in 45.00072169303894 seconds for 528 samples
Epoch: 83 	Training Loss: 0.023053 , computed in 45.226853132247925 seconds for 528 samples
Epoch: 84 	Training Loss: 0.024121 , computed in 44.880340576171875 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.0013552773743867874
Epoch: 85 	Training Loss: 0.024371 , computed in 45.301249980926514 seconds for 528 samples
Epoch: 86 	Training Loss: 0.024518 , computed in 44.88308835029602 seconds for 528 samples
Epoch: 87 	Training Loss: 0.026830 , computed in 45.18187975883484 seconds for 528 samples
Epoch: 88 	Training Loss: 0.024931 , computed in 45.01522731781006 seconds for 528 samples
Epoch: 89 	Training Loss: 0.024805 , computed in 45.253397703170776 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.00278380885720253
wandb: Network error (ConnectionError), entering retry loop.
Epoch: 90 	Training Loss: 0.024842 , computed in 45.51129722595215 seconds for 528 samples
Epoch: 91 	Training Loss: 0.025836 , computed in 45.0503408908844 seconds for 528 samples
Epoch: 92 	Training Loss: 0.024780 , computed in 45.11974787712097 seconds for 528 samples
Epoch: 93 	Training Loss: 0.025460 , computed in 45.429410457611084 seconds for 528 samples
Epoch: 94 	Training Loss: 0.026020 , computed in 45.18713164329529 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.00014683231711387634
Epoch: 95 	Training Loss: 0.024588 , computed in 45.50992560386658 seconds for 528 samples
Epoch: 96 	Training Loss: 0.025183 , computed in 45.37877011299133 seconds for 528 samples
Epoch: 97 	Training Loss: 0.025166 , computed in 45.213194608688354 seconds for 528 samples
Epoch: 98 	Training Loss: 0.025949 , computed in 45.35915207862854 seconds for 528 samples
Epoch: 99 	Training Loss: 0.024283 , computed in 45.42793083190918 seconds for 528 samples
Epoch: 100 	Training Loss: 0.024412 , computed in 44.99362087249756 seconds for 528 samples
wandb: Network error (ConnectionError), entering retry loop.
Epoch: 101 	Training Loss: 0.025940 , computed in 45.34258580207825 seconds for 528 samples
Epoch: 102 	Training Loss: 0.026024 , computed in 45.19195580482483 seconds for 528 samples
Epoch: 103 	Training Loss: 0.023864 , computed in 45.0859649181366 seconds for 528 samples
Epoch: 104 	Training Loss: 0.023613 , computed in 45.25624895095825 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.003819543868303299
Epoch: 105 	Training Loss: 0.025231 , computed in 45.250497579574585 seconds for 528 samples
Epoch: 106 	Training Loss: 0.026921 , computed in 44.70537757873535 seconds for 528 samples
Epoch: 107 	Training Loss: 0.024681 , computed in 45.620017290115356 seconds for 528 samples
Epoch: 108 	Training Loss: 0.025408 , computed in 45.053926944732666 seconds for 528 samples
Epoch: 109 	Training Loss: 0.026505 , computed in 45.02649402618408 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.003166278824210167
Epoch: 110 	Training Loss: 0.024904 , computed in 44.73359823226929 seconds for 528 samples
Epoch: 111 	Training Loss: 0.024761 , computed in 45.489546060562134 seconds for 528 samples
Epoch: 112 	Training Loss: 0.025988 , computed in 44.98603534698486 seconds for 528 samples
Epoch: 113 	Training Loss: 0.025444 , computed in 45.575886487960815 seconds for 528 samples
Epoch: 114 	Training Loss: 0.025170 , computed in 44.68748092651367 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.008093129843473434
Epoch: 115 	Training Loss: 0.024096 , computed in 45.143608808517456 seconds for 528 samples
Epoch: 116 	Training Loss: 0.024715 , computed in 45.42264795303345 seconds for 528 samples
Epoch: 117 	Training Loss: 0.024716 , computed in 44.53738331794739 seconds for 528 samples
Epoch: 118 	Training Loss: 0.024924 , computed in 45.59196639060974 seconds for 528 samples
Epoch: 119 	Training Loss: 0.023358 , computed in 45.03040671348572 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.005240103229880333
Epoch: 120 	Training Loss: 0.024368 , computed in 45.2131609916687 seconds for 528 samples
Epoch: 121 	Training Loss: 0.024837 , computed in 45.29508066177368 seconds for 528 samples
Epoch: 122 	Training Loss: 0.024440 , computed in 45.48651337623596 seconds for 528 samples
Epoch: 123 	Training Loss: 0.025536 , computed in 44.67492961883545 seconds for 528 samples
Epoch: 124 	Training Loss: 0.025808 , computed in 45.023314237594604 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0032937340438365936
Epoch: 125 	Training Loss: 0.023374 , computed in 45.17613410949707 seconds for 528 samples
Epoch: 126 	Training Loss: 0.025081 , computed in 45.28532361984253 seconds for 528 samples
Epoch: 127 	Training Loss: 0.023861 , computed in 45.14803743362427 seconds for 528 samples
Epoch: 128 	Training Loss: 0.025251 , computed in 45.32779812812805 seconds for 528 samples
Epoch: 129 	Training Loss: 0.024340 , computed in 44.657472133636475 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.0010216515511274338
Epoch: 130 	Training Loss: 0.025745 , computed in 45.30713081359863 seconds for 528 samples
Epoch: 131 	Training Loss: 0.025473 , computed in 44.770209074020386 seconds for 528 samples
Epoch: 132 	Training Loss: 0.025750 , computed in 44.913113832473755 seconds for 528 samples
Epoch: 133 	Training Loss: 0.024111 , computed in 45.6480233669281 seconds for 528 samples
Epoch: 134 	Training Loss: 0.023629 , computed in 45.114673376083374 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.0032664798200130463
Epoch: 135 	Training Loss: 0.023119 , computed in 45.254475831985474 seconds for 528 samples
Epoch: 136 	Training Loss: 0.025230 , computed in 45.317816495895386 seconds for 528 samples
Epoch: 137 	Training Loss: 0.025027 , computed in 44.85458564758301 seconds for 528 samples
Epoch: 138 	Training Loss: 0.024875 , computed in 44.876933574676514 seconds for 528 samples
Epoch: 139 	Training Loss: 0.023902 , computed in 45.09842658042908 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.0003878101706504822
Epoch: 140 	Training Loss: 0.024104 , computed in 44.9787495136261 seconds for 528 samples
Epoch: 141 	Training Loss: 0.024838 , computed in 45.16459774971008 seconds for 528 samples
Epoch: 142 	Training Loss: 0.023570 , computed in 45.08328032493591 seconds for 528 samples
Epoch: 143 	Training Loss: 0.026052 , computed in 45.11789608001709 seconds for 528 samples
Epoch: 144 	Training Loss: 0.023964 , computed in 45.0788938999176 seconds for 528 samples
Epoch: 145 	Training Loss: 0.023338 , computed in 45.46202492713928 seconds for 528 samples
Epoch: 146 	Training Loss: 0.024315 , computed in 45.117267370224 seconds for 528 samples
Epoch: 147 	Training Loss: 0.024739 , computed in 45.04985189437866 seconds for 528 samples
Epoch: 148 	Training Loss: 0.023829 , computed in 45.19905138015747 seconds for 528 samples
Epoch: 149 	Training Loss: 0.025147 , computed in 44.97258400917053 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.00879407674074173
Epoch: 150 	Training Loss: 0.024777 , computed in 45.306161880493164 seconds for 528 samples
Epoch: 151 	Training Loss: 0.025095 , computed in 45.28929662704468 seconds for 528 samples
Epoch: 152 	Training Loss: 0.024893 , computed in 45.241689682006836 seconds for 528 samples
Epoch: 153 	Training Loss: 0.026508 , computed in 45.28330683708191 seconds for 528 samples
Epoch: 154 	Training Loss: 0.024009 , computed in 45.46777129173279 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.007901247590780258
Epoch: 155 	Training Loss: 0.023327 , computed in 45.3764123916626 seconds for 528 samples
Epoch: 156 	Training Loss: 0.024033 , computed in 44.95576071739197 seconds for 528 samples
Epoch: 157 	Training Loss: 0.023280 , computed in 45.10572671890259 seconds for 528 samples
Epoch: 158 	Training Loss: 0.024189 , computed in 44.872721672058105 seconds for 528 samples
Epoch: 159 	Training Loss: 0.023871 , computed in 45.34241485595703 seconds for 528 samples
Epoch: 160 	Training Loss: 0.023916 , computed in 45.24657583236694 seconds for 528 samples
Epoch: 161 	Training Loss: 0.023498 , computed in 45.22561311721802 seconds for 528 samples
Epoch: 162 	Training Loss: 0.022726 , computed in 45.141093730926514 seconds for 528 samples
Epoch: 163 	Training Loss: 0.023699 , computed in 45.1258909702301 seconds for 528 samples
Epoch: 164 	Training Loss: 0.025431 , computed in 45.21223473548889 seconds for 528 samples
Epoch: 165 	Training Loss: 0.024767 , computed in 45.2744402885437 seconds for 528 samples
Epoch: 166 	Training Loss: 0.024781 , computed in 44.433777809143066 seconds for 528 samples
Epoch: 167 	Training Loss: 0.024932 , computed in 45.072269916534424 seconds for 528 samples
Epoch: 168 	Training Loss: 0.025001 , computed in 45.407482385635376 seconds for 528 samples
Epoch: 169 	Training Loss: 0.023703 , computed in 44.74491906166077 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0009401850402355194
Epoch: 170 	Training Loss: 0.023840 , computed in 45.22927713394165 seconds for 528 samples
Epoch: 171 	Training Loss: 0.021958 , computed in 45.4967725276947 seconds for 528 samples
Epoch: 172 	Training Loss: 0.024324 , computed in 44.89987301826477 seconds for 528 samples
Epoch: 173 	Training Loss: 0.025366 , computed in 44.83635473251343 seconds for 528 samples
Epoch: 174 	Training Loss: 0.023613 , computed in 45.28783893585205 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.005754994228482246
Epoch: 175 	Training Loss: 0.024508 , computed in 45.04851961135864 seconds for 528 samples
Epoch: 176 	Training Loss: 0.024923 , computed in 45.27628207206726 seconds for 528 samples
Epoch: 177 	Training Loss: 0.023056 , computed in 45.22394514083862 seconds for 528 samples
Epoch: 178 	Training Loss: 0.023540 , computed in 45.214563846588135 seconds for 528 samples
Epoch: 179 	Training Loss: 0.026828 , computed in 45.532480239868164 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.0001151934266090393
Epoch: 180 	Training Loss: 0.023881 , computed in 45.05912160873413 seconds for 528 samples
Epoch: 181 	Training Loss: 0.023871 , computed in 45.34699869155884 seconds for 528 samples
Epoch: 182 	Training Loss: 0.024073 , computed in 45.17188358306885 seconds for 528 samples
Epoch: 183 	Training Loss: 0.024432 , computed in 45.13516068458557 seconds for 528 samples
Epoch: 184 	Training Loss: 0.024329 , computed in 45.27586507797241 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.004324875771999359
Epoch: 185 	Training Loss: 0.025124 , computed in 45.06467890739441 seconds for 528 samples
Epoch: 186 	Training Loss: 0.023144 , computed in 44.975852489471436 seconds for 528 samples
Epoch: 187 	Training Loss: 0.024735 , computed in 44.924312353134155 seconds for 528 samples
Epoch: 188 	Training Loss: 0.023200 , computed in 45.2358832359314 seconds for 528 samples
Epoch: 189 	Training Loss: 0.024307 , computed in 45.263633489608765 seconds for 528 samples
Epoch: 190 	Training Loss: 0.025420 , computed in 45.279194355010986 seconds for 528 samples
Epoch: 191 	Training Loss: 0.022456 , computed in 44.87635898590088 seconds for 528 samples
Epoch: 192 	Training Loss: 0.023537 , computed in 45.37691903114319 seconds for 528 samples
Epoch: 193 	Training Loss: 0.024321 , computed in 45.35325574874878 seconds for 528 samples
Epoch: 194 	Training Loss: 0.024952 , computed in 45.13861417770386 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0010675285011529922
Epoch: 195 	Training Loss: 0.024352 , computed in 45.09227156639099 seconds for 528 samples
Epoch: 196 	Training Loss: 0.024921 , computed in 45.01000690460205 seconds for 528 samples
Epoch: 197 	Training Loss: 0.023750 , computed in 44.73338270187378 seconds for 528 samples
Epoch: 198 	Training Loss: 0.024317 , computed in 44.93739986419678 seconds for 528 samples
Epoch: 199 	Training Loss: 0.024895 , computed in 45.09153604507446 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.0025069620460271835
Epoch: 200 	Training Loss: 0.024175 , computed in 45.72037196159363 seconds for 528 samples
Epoch: 201 	Training Loss: 0.023025 , computed in 44.81538534164429 seconds for 528 samples
Epoch: 202 	Training Loss: 0.022721 , computed in 45.17051863670349 seconds for 528 samples
Epoch: 203 	Training Loss: 0.023776 , computed in 45.11959385871887 seconds for 528 samples
Epoch: 204 	Training Loss: 0.022353 , computed in 45.57415962219238 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.0016791224479675293
Epoch: 205 	Training Loss: 0.025448 , computed in 45.4315299987793 seconds for 528 samples
Epoch: 206 	Training Loss: 0.023773 , computed in 45.16599702835083 seconds for 528 samples
Epoch: 207 	Training Loss: 0.023794 , computed in 45.16763091087341 seconds for 528 samples
Epoch: 208 	Training Loss: 0.023236 , computed in 45.2943069934845 seconds for 528 samples
Epoch: 209 	Training Loss: 0.023941 , computed in 45.256511926651 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.000610915943980217
Epoch: 210 	Training Loss: 0.024787 , computed in 44.670177936553955 seconds for 528 samples
Epoch: 211 	Training Loss: 0.022923 , computed in 45.33465361595154 seconds for 528 samples
Epoch: 212 	Training Loss: 0.023515 , computed in 45.45312738418579 seconds for 528 samples
Epoch: 213 	Training Loss: 0.024090 , computed in 44.978689670562744 seconds for 528 samples
Epoch: 214 	Training Loss: 0.024212 , computed in 45.36708092689514 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.006294179707765579
Epoch: 215 	Training Loss: 0.024590 , computed in 45.27228760719299 seconds for 528 samples
Epoch: 216 	Training Loss: 0.023414 , computed in 44.48594641685486 seconds for 528 samples
Epoch: 217 	Training Loss: 0.023303 , computed in 45.36197066307068 seconds for 528 samples
Epoch: 218 	Training Loss: 0.022998 , computed in 45.22571587562561 seconds for 528 samples
Epoch: 219 	Training Loss: 0.023485 , computed in 45.13984155654907 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.018324725329875946
Epoch: 220 	Training Loss: 0.023209 , computed in 45.0318877696991 seconds for 528 samples
Epoch: 221 	Training Loss: 0.023739 , computed in 45.04946184158325 seconds for 528 samples
Epoch: 222 	Training Loss: 0.024049 , computed in 44.90786552429199 seconds for 528 samples
Epoch: 223 	Training Loss: 0.023609 , computed in 45.33233714103699 seconds for 528 samples
Epoch: 224 	Training Loss: 0.022258 , computed in 45.11897397041321 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.007429921999573708
Epoch: 225 	Training Loss: 0.024086 , computed in 45.15930128097534 seconds for 528 samples
Epoch: 226 	Training Loss: 0.023539 , computed in 45.130544662475586 seconds for 528 samples
Epoch: 227 	Training Loss: 0.024620 , computed in 45.398359298706055 seconds for 528 samples
Epoch: 228 	Training Loss: 0.024136 , computed in 45.3118999004364 seconds for 528 samples
Epoch: 229 	Training Loss: 0.024769 , computed in 45.090409994125366 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.00015821680426597595
Epoch: 230 	Training Loss: 0.023042 , computed in 45.4110209941864 seconds for 528 samples
Epoch: 231 	Training Loss: 0.024029 , computed in 45.02843499183655 seconds for 528 samples
Epoch: 232 	Training Loss: 0.023317 , computed in 44.83497381210327 seconds for 528 samples
Epoch: 233 	Training Loss: 0.024025 , computed in 44.73372483253479 seconds for 528 samples
Epoch: 234 	Training Loss: 0.023987 , computed in 45.62875032424927 seconds for 528 samples
INFO: Early stopping counter 9 of 1500 with -0.0014577154070138931
Epoch: 235 	Training Loss: 0.023829 , computed in 45.71895956993103 seconds for 528 samples
Epoch: 236 	Training Loss: 0.024615 , computed in 45.271454095840454 seconds for 528 samples
Epoch: 237 	Training Loss: 0.023796 , computed in 44.92327427864075 seconds for 528 samples
Epoch: 238 	Training Loss: 0.023926 , computed in 45.354676961898804 seconds for 528 samples
Epoch: 239 	Training Loss: 0.024025 , computed in 45.23664712905884 seconds for 528 samples
INFO: Early stopping counter 10 of 1500 with -0.0020903311669826508
Epoch: 240 	Training Loss: 0.025369 , computed in 45.27381086349487 seconds for 528 samples
Epoch: 241 	Training Loss: 0.022975 , computed in 45.38624143600464 seconds for 528 samples
Epoch: 242 	Training Loss: 0.024324 , computed in 45.10138773918152 seconds for 528 samples
Epoch: 243 	Training Loss: 0.022442 , computed in 45.77888560295105 seconds for 528 samples
Epoch: 244 	Training Loss: 0.023773 , computed in 45.3726224899292 seconds for 528 samples
INFO: Early stopping counter 11 of 1500 with -0.006869994103908539
Epoch: 245 	Training Loss: 0.024608 , computed in 45.32903861999512 seconds for 528 samples
Epoch: 246 	Training Loss: 0.024432 , computed in 45.47843885421753 seconds for 528 samples
Epoch: 247 	Training Loss: 0.023508 , computed in 45.300480365753174 seconds for 528 samples
Epoch: 248 	Training Loss: 0.022646 , computed in 45.139949798583984 seconds for 528 samples
Epoch: 249 	Training Loss: 0.021736 , computed in 45.73273301124573 seconds for 528 samples
INFO: Early stopping counter 12 of 1500 with -0.006068523973226547
Epoch: 250 	Training Loss: 0.021938 , computed in 45.0751428604126 seconds for 528 samples
Epoch: 251 	Training Loss: 0.023422 , computed in 45.13097262382507 seconds for 528 samples
Epoch: 252 	Training Loss: 0.024267 , computed in 45.25271987915039 seconds for 528 samples
Epoch: 253 	Training Loss: 0.024897 , computed in 44.92989468574524 seconds for 528 samples
Epoch: 254 	Training Loss: 0.023550 , computed in 45.271437644958496 seconds for 528 samples
INFO: Early stopping counter 13 of 1500 with -0.005242103710770607
Epoch: 255 	Training Loss: 0.024941 , computed in 45.44175910949707 seconds for 528 samples
Epoch: 256 	Training Loss: 0.023504 , computed in 45.26750349998474 seconds for 528 samples
Epoch: 257 	Training Loss: 0.022142 , computed in 44.75435137748718 seconds for 528 samples
Epoch: 258 	Training Loss: 0.023622 , computed in 45.22170090675354 seconds for 528 samples
Epoch: 259 	Training Loss: 0.024963 , computed in 45.04606246948242 seconds for 528 samples
Epoch: 260 	Training Loss: 0.022939 , computed in 45.32742738723755 seconds for 528 samples
Epoch: 261 	Training Loss: 0.023653 , computed in 44.98764967918396 seconds for 528 samples
Epoch: 262 	Training Loss: 0.021986 , computed in 45.08288931846619 seconds for 528 samples
Epoch: 263 	Training Loss: 0.024277 , computed in 45.415576457977295 seconds for 528 samples
Epoch: 264 	Training Loss: 0.024451 , computed in 45.35534453392029 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.008404361084103584
Epoch: 265 	Training Loss: 0.024099 , computed in 45.072919845581055 seconds for 528 samples
Epoch: 266 	Training Loss: 0.022904 , computed in 45.65261125564575 seconds for 528 samples
Epoch: 267 	Training Loss: 0.024374 , computed in 45.04388952255249 seconds for 528 samples
Epoch: 268 	Training Loss: 0.022993 , computed in 45.837785482406616 seconds for 528 samples
Epoch: 269 	Training Loss: 0.022916 , computed in 45.39227271080017 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.004769047722220421
Epoch: 270 	Training Loss: 0.023244 , computed in 45.00750017166138 seconds for 528 samples
Epoch: 271 	Training Loss: 0.023436 , computed in 45.21909856796265 seconds for 528 samples
Epoch: 272 	Training Loss: 0.022429 , computed in 45.009521484375 seconds for 528 samples
Epoch: 273 	Training Loss: 0.023127 , computed in 45.34802484512329 seconds for 528 samples
Epoch: 274 	Training Loss: 0.024148 , computed in 45.32965302467346 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.004067247733473778
Epoch: 275 	Training Loss: 0.023253 , computed in 45.581732511520386 seconds for 528 samples
Epoch: 276 	Training Loss: 0.023155 , computed in 45.381725788116455 seconds for 528 samples
Epoch: 277 	Training Loss: 0.022695 , computed in 44.79409599304199 seconds for 528 samples
Epoch: 278 	Training Loss: 0.021663 , computed in 45.02274799346924 seconds for 528 samples
Epoch: 279 	Training Loss: 0.025317 , computed in 45.406604528427124 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.0038364846259355545
Epoch: 280 	Training Loss: 0.022800 , computed in 45.39624810218811 seconds for 528 samples
Epoch: 281 	Training Loss: 0.024522 , computed in 45.20049285888672 seconds for 528 samples
Epoch: 282 	Training Loss: 0.022911 , computed in 45.12842392921448 seconds for 528 samples
Epoch: 283 	Training Loss: 0.023805 , computed in 45.45382332801819 seconds for 528 samples
Epoch: 284 	Training Loss: 0.023083 , computed in 45.39969992637634 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0035868659615516663
Epoch: 285 	Training Loss: 0.024569 , computed in 45.02082133293152 seconds for 528 samples
Epoch: 286 	Training Loss: 0.023051 , computed in 44.83374309539795 seconds for 528 samples
Epoch: 287 	Training Loss: 0.021881 , computed in 44.861111879348755 seconds for 528 samples
Epoch: 288 	Training Loss: 0.023831 , computed in 45.49199604988098 seconds for 528 samples
Epoch: 289 	Training Loss: 0.022717 , computed in 44.97057867050171 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.0011282730847597122
Epoch: 290 	Training Loss: 0.023736 , computed in 45.10787796974182 seconds for 528 samples
Epoch: 291 	Training Loss: 0.023791 , computed in 45.46143126487732 seconds for 528 samples
Epoch: 292 	Training Loss: 0.025559 , computed in 45.011746883392334 seconds for 528 samples
Epoch: 293 	Training Loss: 0.023600 , computed in 45.07021641731262 seconds for 528 samples
Epoch: 294 	Training Loss: 0.023903 , computed in 45.307180404663086 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.002110486850142479
Epoch: 295 	Training Loss: 0.023188 , computed in 45.782453298568726 seconds for 528 samples
Epoch: 296 	Training Loss: 0.022765 , computed in 45.159682273864746 seconds for 528 samples
Epoch: 297 	Training Loss: 0.023163 , computed in 45.40328502655029 seconds for 528 samples
Epoch: 298 	Training Loss: 0.022985 , computed in 45.42057490348816 seconds for 528 samples
Epoch: 299 	Training Loss: 0.024039 , computed in 45.38778233528137 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.007354559376835823
Epoch: 300 	Training Loss: 0.022783 , computed in 45.81769895553589 seconds for 528 samples
Epoch: 301 	Training Loss: 0.023856 , computed in 44.98598313331604 seconds for 528 samples
Epoch: 302 	Training Loss: 0.024178 , computed in 45.415343284606934 seconds for 528 samples
Epoch: 303 	Training Loss: 0.023501 , computed in 45.059964179992676 seconds for 528 samples
Epoch: 304 	Training Loss: 0.023722 , computed in 45.41948890686035 seconds for 528 samples
INFO: Early stopping counter 9 of 1500 with -0.000889861024916172
Epoch: 305 	Training Loss: 0.023032 , computed in 45.07485747337341 seconds for 528 samples
Epoch: 306 	Training Loss: 0.023365 , computed in 44.82436752319336 seconds for 528 samples
Epoch: 307 	Training Loss: 0.023155 , computed in 44.96821069717407 seconds for 528 samples
Epoch: 308 	Training Loss: 0.023410 , computed in 45.60407590866089 seconds for 528 samples
Epoch: 309 	Training Loss: 0.024463 , computed in 45.198442459106445 seconds for 528 samples
INFO: Early stopping counter 10 of 1500 with -0.0013436097651720047
Epoch: 310 	Training Loss: 0.023251 , computed in 44.991042375564575 seconds for 528 samples
Epoch: 311 	Training Loss: 0.021602 , computed in 45.236090898513794 seconds for 528 samples
Epoch: 312 	Training Loss: 0.025056 , computed in 44.813876152038574 seconds for 528 samples
Epoch: 313 	Training Loss: 0.023258 , computed in 45.31710624694824 seconds for 528 samples
Epoch: 314 	Training Loss: 0.022787 , computed in 44.980714321136475 seconds for 528 samples
INFO: Early stopping counter 11 of 1500 with -0.00757715106010437
Epoch: 315 	Training Loss: 0.023787 , computed in 45.21997332572937 seconds for 528 samples
Epoch: 316 	Training Loss: 0.023478 , computed in 45.36194658279419 seconds for 528 samples
Epoch: 317 	Training Loss: 0.022687 , computed in 45.42808985710144 seconds for 528 samples
Epoch: 318 	Training Loss: 0.024948 , computed in 45.36422514915466 seconds for 528 samples
Epoch: 319 	Training Loss: 0.024024 , computed in 45.440608739852905 seconds for 528 samples
INFO: Early stopping counter 12 of 1500 with -0.001567034050822258
Epoch: 320 	Training Loss: 0.022524 , computed in 45.31319069862366 seconds for 528 samples
Epoch: 321 	Training Loss: 0.023683 , computed in 45.595834493637085 seconds for 528 samples
Epoch: 322 	Training Loss: 0.023074 , computed in 45.03810930252075 seconds for 528 samples
Epoch: 323 	Training Loss: 0.023126 , computed in 45.43957805633545 seconds for 528 samples
Epoch: 324 	Training Loss: 0.022762 , computed in 45.08769989013672 seconds for 528 samples
INFO: Early stopping counter 13 of 1500 with -0.002480544149875641
Epoch: 325 	Training Loss: 0.023640 , computed in 45.290327310562134 seconds for 528 samples
Epoch: 326 	Training Loss: 0.023428 , computed in 45.17689514160156 seconds for 528 samples
Epoch: 327 	Training Loss: 0.023431 , computed in 45.0069317817688 seconds for 528 samples
Epoch: 328 	Training Loss: 0.023480 , computed in 45.261730432510376 seconds for 528 samples
Epoch: 329 	Training Loss: 0.022789 , computed in 45.27769088745117 seconds for 528 samples
INFO: Early stopping counter 14 of 1500 with -0.0007533356547355652
Epoch: 330 	Training Loss: 0.023309 , computed in 45.12341260910034 seconds for 528 samples
Epoch: 331 	Training Loss: 0.023620 , computed in 45.529706716537476 seconds for 528 samples
Epoch: 332 	Training Loss: 0.022334 , computed in 45.300950050354004 seconds for 528 samples
Epoch: 333 	Training Loss: 0.025352 , computed in 45.34601926803589 seconds for 528 samples
Epoch: 334 	Training Loss: 0.023276 , computed in 45.08182430267334 seconds for 528 samples
INFO: Early stopping counter 15 of 1500 with -0.003175610676407814
Epoch: 335 	Training Loss: 0.023446 , computed in 45.216203689575195 seconds for 528 samples
Epoch: 336 	Training Loss: 0.023615 , computed in 45.380533933639526 seconds for 528 samples
Epoch: 337 	Training Loss: 0.022572 , computed in 45.117599964141846 seconds for 528 samples
Epoch: 338 	Training Loss: 0.023484 , computed in 44.996097564697266 seconds for 528 samples
Epoch: 339 	Training Loss: 0.024001 , computed in 45.291741609573364 seconds for 528 samples
INFO: Early stopping counter 16 of 1500 with -0.0008708164095878601
Epoch: 340 	Training Loss: 0.022582 , computed in 45.342142820358276 seconds for 528 samples
Epoch: 341 	Training Loss: 0.023214 , computed in 45.366570472717285 seconds for 528 samples
Epoch: 342 	Training Loss: 0.023080 , computed in 45.30793762207031 seconds for 528 samples
Epoch: 343 	Training Loss: 0.021848 , computed in 45.37586998939514 seconds for 528 samples
Epoch: 344 	Training Loss: 0.023963 , computed in 44.98433542251587 seconds for 528 samples
INFO: Early stopping counter 17 of 1500 with -0.010456318035721779
Epoch: 345 	Training Loss: 0.023445 , computed in 45.26610088348389 seconds for 528 samples
Epoch: 346 	Training Loss: 0.022882 , computed in 45.44571495056152 seconds for 528 samples
Epoch: 347 	Training Loss: 0.021901 , computed in 45.031848669052124 seconds for 528 samples
Epoch: 348 	Training Loss: 0.023591 , computed in 45.02069044113159 seconds for 528 samples
Epoch: 349 	Training Loss: 0.022984 , computed in 45.561936140060425 seconds for 528 samples
INFO: Early stopping counter 18 of 1500 with -0.0008253082633018494
Epoch: 350 	Training Loss: 0.022291 , computed in 45.357582330703735 seconds for 528 samples
Epoch: 351 	Training Loss: 0.024429 , computed in 44.81584024429321 seconds for 528 samples
Epoch: 352 	Training Loss: 0.023589 , computed in 44.62718892097473 seconds for 528 samples
Epoch: 353 	Training Loss: 0.022190 , computed in 45.28732109069824 seconds for 528 samples
Epoch: 354 	Training Loss: 0.023080 , computed in 45.174787521362305 seconds for 528 samples
INFO: Early stopping counter 19 of 1500 with -0.000379231758415699
Epoch: 355 	Training Loss: 0.023072 , computed in 45.09112000465393 seconds for 528 samples
Epoch: 356 	Training Loss: 0.022540 , computed in 44.80358910560608 seconds for 528 samples
Epoch: 357 	Training Loss: 0.024129 , computed in 45.205013036727905 seconds for 528 samples
Epoch: 358 	Training Loss: 0.022995 , computed in 44.89534306526184 seconds for 528 samples
Epoch: 359 	Training Loss: 0.023404 , computed in 45.35501670837402 seconds for 528 samples
INFO: Early stopping counter 20 of 1500 with -0.0007537603378295898
Epoch: 360 	Training Loss: 0.022228 , computed in 44.98326563835144 seconds for 528 samples
Epoch: 361 	Training Loss: 0.023392 , computed in 45.26892423629761 seconds for 528 samples
Epoch: 362 	Training Loss: 0.025090 , computed in 44.7285270690918 seconds for 528 samples
Epoch: 363 	Training Loss: 0.021679 , computed in 45.25237989425659 seconds for 528 samples
Epoch: 364 	Training Loss: 0.022533 , computed in 44.6993522644043 seconds for 528 samples
INFO: Early stopping counter 21 of 1500 with -0.0008118394762277603
Epoch: 365 	Training Loss: 0.022165 , computed in 45.50681805610657 seconds for 528 samples
Epoch: 366 	Training Loss: 0.022109 , computed in 44.8613703250885 seconds for 528 samples
Epoch: 367 	Training Loss: 0.023234 , computed in 45.2355010509491 seconds for 528 samples
Epoch: 368 	Training Loss: 0.023769 , computed in 45.109034299850464 seconds for 528 samples
Epoch: 369 	Training Loss: 0.023447 , computed in 45.318949699401855 seconds for 528 samples
INFO: Early stopping counter 22 of 1500 with -0.006014319136738777
Epoch: 370 	Training Loss: 0.025141 , computed in 45.320258140563965 seconds for 528 samples
Epoch: 371 	Training Loss: 0.024192 , computed in 44.96111989021301 seconds for 528 samples
Epoch: 372 	Training Loss: 0.025292 , computed in 45.243515968322754 seconds for 528 samples
Epoch: 373 	Training Loss: 0.021458 , computed in 45.20828676223755 seconds for 528 samples
Epoch: 374 	Training Loss: 0.023835 , computed in 45.43012285232544 seconds for 528 samples
INFO: Early stopping counter 23 of 1500 with -0.008251942694187164
Epoch: 375 	Training Loss: 0.021784 , computed in 45.33492851257324 seconds for 528 samples
Epoch: 376 	Training Loss: 0.024994 , computed in 45.26180410385132 seconds for 528 samples
Epoch: 377 	Training Loss: 0.025118 , computed in 45.02623748779297 seconds for 528 samples
Epoch: 378 	Training Loss: 0.023660 , computed in 45.19249653816223 seconds for 528 samples
Epoch: 379 	Training Loss: 0.022462 , computed in 45.24897027015686 seconds for 528 samples
Epoch: 380 	Training Loss: 0.024001 , computed in 44.991302490234375 seconds for 528 samples
Epoch: 381 	Training Loss: 0.023687 , computed in 45.53173089027405 seconds for 528 samples
Epoch: 382 	Training Loss: 0.022817 , computed in 45.10637927055359 seconds for 528 samples
Epoch: 383 	Training Loss: 0.023678 , computed in 45.35704469680786 seconds for 528 samples
Epoch: 384 	Training Loss: 0.022743 , computed in 44.937912702560425 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0024784784764051437
Epoch: 385 	Training Loss: 0.025435 , computed in 45.219096422195435 seconds for 528 samples
Epoch: 386 	Training Loss: 0.022328 , computed in 45.0204815864563 seconds for 528 samples
Epoch: 387 	Training Loss: 0.024235 , computed in 45.277788400650024 seconds for 528 samples
Epoch: 388 	Training Loss: 0.023369 , computed in 45.26953053474426 seconds for 528 samples
Epoch: 389 	Training Loss: 0.023213 , computed in 44.75236535072327 seconds for 528 samples
Epoch: 390 	Training Loss: 0.025224 , computed in 45.52925515174866 seconds for 528 samples
Epoch: 391 	Training Loss: 0.023812 , computed in 45.231284379959106 seconds for 528 samples
Epoch: 392 	Training Loss: 0.023094 , computed in 45.19733905792236 seconds for 528 samples
Epoch: 393 	Training Loss: 0.023558 , computed in 44.72300171852112 seconds for 528 samples
Epoch: 394 	Training Loss: 0.024940 , computed in 45.26067781448364 seconds for 528 samples
Epoch: 395 	Training Loss: 0.023741 , computed in 44.933276891708374 seconds for 528 samples
Epoch: 396 	Training Loss: 0.023839 , computed in 45.2170352935791 seconds for 528 samples
Epoch: 397 	Training Loss: 0.022890 , computed in 45.0453200340271 seconds for 528 samples
Epoch: 398 	Training Loss: 0.022236 , computed in 45.256733894348145 seconds for 528 samples
Epoch: 399 	Training Loss: 0.024308 , computed in 45.36125564575195 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.002743072807788849
Epoch: 400 	Training Loss: 0.022748 , computed in 45.27607202529907 seconds for 528 samples
Epoch: 401 	Training Loss: 0.023896 , computed in 45.07862043380737 seconds for 528 samples
Epoch: 402 	Training Loss: 0.022027 , computed in 45.156481981277466 seconds for 528 samples
Epoch: 403 	Training Loss: 0.023246 , computed in 45.27976059913635 seconds for 528 samples
Epoch: 404 	Training Loss: 0.023586 , computed in 45.16593551635742 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.0035042837262153625
Epoch: 405 	Training Loss: 0.022657 , computed in 44.8295419216156 seconds for 528 samples
Epoch: 406 	Training Loss: 0.023080 , computed in 44.92205286026001 seconds for 528 samples
Epoch: 407 	Training Loss: 0.024228 , computed in 45.45322275161743 seconds for 528 samples
Epoch: 408 	Training Loss: 0.023540 , computed in 44.88276696205139 seconds for 528 samples
Epoch: 409 	Training Loss: 0.022737 , computed in 45.55511546134949 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.00601399689912796
Epoch: 410 	Training Loss: 0.022494 , computed in 45.067694425582886 seconds for 528 samples
Epoch: 411 	Training Loss: 0.022684 , computed in 44.927019357681274 seconds for 528 samples
Epoch: 412 	Training Loss: 0.024287 , computed in 44.885517597198486 seconds for 528 samples
Epoch: 413 	Training Loss: 0.022852 , computed in 45.19780135154724 seconds for 528 samples
Epoch: 414 	Training Loss: 0.023316 , computed in 45.400572538375854 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.00013780314475297928
Epoch: 415 	Training Loss: 0.023798 , computed in 45.185150384902954 seconds for 528 samples
Epoch: 416 	Training Loss: 0.021579 , computed in 45.0573525428772 seconds for 528 samples
Epoch: 417 	Training Loss: 0.023080 , computed in 44.82617521286011 seconds for 528 samples
Epoch: 418 	Training Loss: 0.022540 , computed in 45.384581565856934 seconds for 528 samples
Epoch: 419 	Training Loss: 0.022022 , computed in 45.018221616744995 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0006458871066570282
Epoch: 420 	Training Loss: 0.023390 , computed in 45.23214817047119 seconds for 528 samples
Epoch: 421 	Training Loss: 0.023821 , computed in 45.11354660987854 seconds for 528 samples
Epoch: 422 	Training Loss: 0.021126 , computed in 45.119558572769165 seconds for 528 samples
Epoch: 423 	Training Loss: 0.023024 , computed in 45.12614560127258 seconds for 528 samples
Epoch: 424 	Training Loss: 0.023220 , computed in 45.4797306060791 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.0012023970484733582
Epoch: 425 	Training Loss: 0.022938 , computed in 45.48116111755371 seconds for 528 samples
Epoch: 426 	Training Loss: 0.022915 , computed in 44.99377679824829 seconds for 528 samples
Epoch: 427 	Training Loss: 0.023620 , computed in 45.34875273704529 seconds for 528 samples
Epoch: 428 	Training Loss: 0.023640 , computed in 45.33086657524109 seconds for 528 samples
Epoch: 429 	Training Loss: 0.023932 , computed in 45.14532661437988 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.0013415487483143806
Epoch: 430 	Training Loss: 0.023830 , computed in 45.2738573551178 seconds for 528 samples
Epoch: 431 	Training Loss: 0.023125 , computed in 45.288477420806885 seconds for 528 samples
Epoch: 432 	Training Loss: 0.023201 , computed in 45.33617615699768 seconds for 528 samples
Epoch: 433 	Training Loss: 0.021481 , computed in 45.00085091590881 seconds for 528 samples
Epoch: 434 	Training Loss: 0.022391 , computed in 45.16089487075806 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.002568798139691353
Epoch: 435 	Training Loss: 0.021786 , computed in 45.49004507064819 seconds for 528 samples
Epoch: 436 	Training Loss: 0.023887 , computed in 45.12433433532715 seconds for 528 samples
Epoch: 437 	Training Loss: 0.023503 , computed in 45.23830509185791 seconds for 528 samples
Epoch: 438 	Training Loss: 0.023898 , computed in 45.20892024040222 seconds for 528 samples
Epoch: 439 	Training Loss: 0.023071 , computed in 45.42957162857056 seconds for 528 samples
INFO: Early stopping counter 9 of 1500 with -0.0008014347404241562
Epoch: 440 	Training Loss: 0.022091 , computed in 45.241445779800415 seconds for 528 samples
Epoch: 441 	Training Loss: 0.023223 , computed in 44.84190535545349 seconds for 528 samples
Epoch: 442 	Training Loss: 0.023933 , computed in 44.96542572975159 seconds for 528 samples
Epoch: 443 	Training Loss: 0.022395 , computed in 45.41350960731506 seconds for 528 samples
Epoch: 444 	Training Loss: 0.023242 , computed in 45.242496490478516 seconds for 528 samples
INFO: Early stopping counter 10 of 1500 with -0.0032070372253656387
Epoch: 445 	Training Loss: 0.023263 , computed in 45.40843486785889 seconds for 528 samples
Epoch: 446 	Training Loss: 0.024226 , computed in 44.9936261177063 seconds for 528 samples
Epoch: 447 	Training Loss: 0.023404 , computed in 45.40656900405884 seconds for 528 samples
Epoch: 448 	Training Loss: 0.024744 , computed in 45.12419319152832 seconds for 528 samples
Epoch: 449 	Training Loss: 0.022236 , computed in 45.49326491355896 seconds for 528 samples
INFO: Early stopping counter 11 of 1500 with -0.0003661084920167923
Epoch: 450 	Training Loss: 0.022770 , computed in 45.28271770477295 seconds for 528 samples
Epoch: 451 	Training Loss: 0.022835 , computed in 45.34294390678406 seconds for 528 samples
Epoch: 452 	Training Loss: 0.023134 , computed in 45.26588702201843 seconds for 528 samples
Epoch: 453 	Training Loss: 0.022872 , computed in 45.42487096786499 seconds for 528 samples
Epoch: 454 	Training Loss: 0.022872 , computed in 46.10573697090149 seconds for 528 samples
INFO: Early stopping counter 12 of 1500 with -0.0025795791298151016
Epoch: 455 	Training Loss: 0.024311 , computed in 45.25356316566467 seconds for 528 samples
Epoch: 456 	Training Loss: 0.022941 , computed in 44.90998697280884 seconds for 528 samples
Epoch: 457 	Training Loss: 0.022509 , computed in 45.271146059036255 seconds for 528 samples
Epoch: 458 	Training Loss: 0.023361 , computed in 45.30724477767944 seconds for 528 samples
Epoch: 459 	Training Loss: 0.023157 , computed in 45.399561166763306 seconds for 528 samples
Epoch: 460 	Training Loss: 0.023800 , computed in 45.40169310569763 seconds for 528 samples
Epoch: 461 	Training Loss: 0.021424 , computed in 45.387308835983276 seconds for 528 samples
Epoch: 462 	Training Loss: 0.022504 , computed in 45.43580508232117 seconds for 528 samples
Epoch: 463 	Training Loss: 0.023520 , computed in 45.1752188205719 seconds for 528 samples
Epoch: 464 	Training Loss: 0.022429 , computed in 45.3978488445282 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0009434744715690613
Epoch: 465 	Training Loss: 0.023103 , computed in 45.16258263587952 seconds for 528 samples
Epoch: 466 	Training Loss: 0.022343 , computed in 45.19352626800537 seconds for 528 samples
Epoch: 467 	Training Loss: 0.022732 , computed in 45.48440909385681 seconds for 528 samples
Epoch: 468 	Training Loss: 0.023152 , computed in 44.916860580444336 seconds for 528 samples
Epoch: 469 	Training Loss: 0.023819 , computed in 45.326574087142944 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.001625746488571167
Epoch: 470 	Training Loss: 0.022138 , computed in 45.30580949783325 seconds for 528 samples
Epoch: 471 	Training Loss: 0.023928 , computed in 44.88842582702637 seconds for 528 samples
Epoch: 472 	Training Loss: 0.023024 , computed in 45.001953125 seconds for 528 samples
Epoch: 473 	Training Loss: 0.023721 , computed in 45.28551888465881 seconds for 528 samples
Epoch: 474 	Training Loss: 0.022817 , computed in 45.03268098831177 seconds for 528 samples
Epoch: 475 	Training Loss: 0.023628 , computed in 45.422677755355835 seconds for 528 samples
Epoch: 476 	Training Loss: 0.023612 , computed in 44.95802116394043 seconds for 528 samples
Epoch: 477 	Training Loss: 0.021687 , computed in 45.430469036102295 seconds for 528 samples
Epoch: 478 	Training Loss: 0.023360 , computed in 45.28356432914734 seconds for 528 samples
Epoch: 479 	Training Loss: 0.022848 , computed in 45.25753450393677 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0036126505583524704
Epoch: 480 	Training Loss: 0.022145 , computed in 45.07679772377014 seconds for 528 samples
Epoch: 481 	Training Loss: 0.023205 , computed in 45.32363152503967 seconds for 528 samples
Epoch: 482 	Training Loss: 0.024041 , computed in 44.93674421310425 seconds for 528 samples
Epoch: 483 	Training Loss: 0.021609 , computed in 45.383944272994995 seconds for 528 samples
Epoch: 484 	Training Loss: 0.024408 , computed in 44.90113162994385 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.007150096818804741
Epoch: 485 	Training Loss: 0.023339 , computed in 45.29290294647217 seconds for 528 samples
Epoch: 486 	Training Loss: 0.022730 , computed in 44.93643760681152 seconds for 528 samples
Epoch: 487 	Training Loss: 0.022304 , computed in 45.03613829612732 seconds for 528 samples
Epoch: 488 	Training Loss: 0.022451 , computed in 45.35347056388855 seconds for 528 samples
Epoch: 489 	Training Loss: 0.022875 , computed in 45.299710512161255 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.0009240936487913132
Epoch: 490 	Training Loss: 0.022637 , computed in 45.37032413482666 seconds for 528 samples
Epoch: 491 	Training Loss: 0.023012 , computed in 44.996001958847046 seconds for 528 samples
Epoch: 492 	Training Loss: 0.021975 , computed in 44.76640844345093 seconds for 528 samples
Epoch: 493 	Training Loss: 0.022692 , computed in 45.23561954498291 seconds for 528 samples
Epoch: 494 	Training Loss: 0.022023 , computed in 45.419580698013306 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.0021440479904413223
Epoch: 495 	Training Loss: 0.023542 , computed in 45.578718185424805 seconds for 528 samples
Epoch: 496 	Training Loss: 0.022189 , computed in 45.26871347427368 seconds for 528 samples
Epoch: 497 	Training Loss: 0.024339 , computed in 45.11851525306702 seconds for 528 samples
Epoch: 498 	Training Loss: 0.023008 , computed in 45.016582012176514 seconds for 528 samples
Epoch: 499 	Training Loss: 0.023264 , computed in 45.60097575187683 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0010459031909704208
Epoch: 500 	Training Loss: 0.022219 , computed in 45.381073236465454 seconds for 528 samples
Epoch: 501 	Training Loss: 0.022987 , computed in 45.33871364593506 seconds for 528 samples
Epoch: 502 	Training Loss: 0.023188 , computed in 45.355796337127686 seconds for 528 samples
Epoch: 503 	Training Loss: 0.022317 , computed in 45.73862600326538 seconds for 528 samples
Epoch: 504 	Training Loss: 0.023000 , computed in 45.259230852127075 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.003920378163456917
Epoch: 505 	Training Loss: 0.023489 , computed in 45.487953662872314 seconds for 528 samples
Epoch: 506 	Training Loss: 0.022106 , computed in 45.09434199333191 seconds for 528 samples
Epoch: 507 	Training Loss: 0.022729 , computed in 45.51431488990784 seconds for 528 samples
Epoch: 508 	Training Loss: 0.022674 , computed in 45.212931871414185 seconds for 528 samples
Epoch: 509 	Training Loss: 0.023074 , computed in 45.36361885070801 seconds for 528 samples
Epoch: 510 	Training Loss: 0.023149 , computed in 45.342854022979736 seconds for 528 samples
Epoch: 511 	Training Loss: 0.023029 , computed in 45.311790227890015 seconds for 528 samples
Epoch: 512 	Training Loss: 0.023207 , computed in 45.393081188201904 seconds for 528 samples
Epoch: 513 	Training Loss: 0.021566 , computed in 45.15582823753357 seconds for 528 samples
Epoch: 514 	Training Loss: 0.023162 , computed in 45.37264442443848 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0030610505491495132
Epoch: 515 	Training Loss: 0.024018 , computed in 45.53655290603638 seconds for 528 samples
Epoch: 516 	Training Loss: 0.022936 , computed in 45.367326736450195 seconds for 528 samples
Epoch: 517 	Training Loss: 0.023358 , computed in 45.27188062667847 seconds for 528 samples
Epoch: 518 	Training Loss: 0.023204 , computed in 45.20218014717102 seconds for 528 samples
Epoch: 519 	Training Loss: 0.022251 , computed in 45.25128436088562 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.00044705532491207123
Epoch: 520 	Training Loss: 0.023150 , computed in 45.21235132217407 seconds for 528 samples
Epoch: 521 	Training Loss: 0.023659 , computed in 45.28626108169556 seconds for 528 samples
Epoch: 522 	Training Loss: 0.023799 , computed in 45.88908791542053 seconds for 528 samples
Epoch: 523 	Training Loss: 0.022651 , computed in 44.90120506286621 seconds for 528 samples
Epoch: 524 	Training Loss: 0.022857 , computed in 45.14577102661133 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.0021989867091178894
Epoch: 525 	Training Loss: 0.022469 , computed in 45.38020396232605 seconds for 528 samples
Epoch: 526 	Training Loss: 0.022820 , computed in 45.209643602371216 seconds for 528 samples
Epoch: 527 	Training Loss: 0.023326 , computed in 45.17079019546509 seconds for 528 samples
Epoch: 528 	Training Loss: 0.023562 , computed in 45.117042541503906 seconds for 528 samples
Epoch: 529 	Training Loss: 0.022489 , computed in 45.41853857040405 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.005823047831654549
Epoch: 530 	Training Loss: 0.026224 , computed in 45.229058265686035 seconds for 528 samples
Epoch: 531 	Training Loss: 0.022701 , computed in 44.80521035194397 seconds for 528 samples
Epoch: 532 	Training Loss: 0.021742 , computed in 44.98677372932434 seconds for 528 samples
Epoch: 533 	Training Loss: 0.024606 , computed in 45.43662214279175 seconds for 528 samples
Epoch: 534 	Training Loss: 0.023158 , computed in 45.45367217063904 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0022227074950933456
Epoch: 535 	Training Loss: 0.023203 , computed in 45.37095046043396 seconds for 528 samples
Epoch: 536 	Training Loss: 0.022627 , computed in 44.941184997558594 seconds for 528 samples
Epoch: 537 	Training Loss: 0.022967 , computed in 45.40868139266968 seconds for 528 samples
Epoch: 538 	Training Loss: 0.023748 , computed in 45.52893662452698 seconds for 528 samples
Epoch: 539 	Training Loss: 0.022382 , computed in 45.23609185218811 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.0011806869879364967
Epoch: 540 	Training Loss: 0.021894 , computed in 45.10459327697754 seconds for 528 samples
Epoch: 541 	Training Loss: 0.023761 , computed in 45.11245036125183 seconds for 528 samples
Epoch: 542 	Training Loss: 0.021669 , computed in 45.02148771286011 seconds for 528 samples
Epoch: 543 	Training Loss: 0.023175 , computed in 45.32245969772339 seconds for 528 samples
Epoch: 544 	Training Loss: 0.022790 , computed in 45.14571237564087 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.002823883667588234
Epoch: 545 	Training Loss: 0.022673 , computed in 45.35552263259888 seconds for 528 samples
Epoch: 546 	Training Loss: 0.022264 , computed in 45.091747999191284 seconds for 528 samples
Epoch: 547 	Training Loss: 0.021795 , computed in 45.027196645736694 seconds for 528 samples
Epoch: 548 	Training Loss: 0.023132 , computed in 44.908276319503784 seconds for 528 samples
Epoch: 549 	Training Loss: 0.022209 , computed in 45.17815923690796 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.0044615790247917175
Epoch: 550 	Training Loss: 0.021760 , computed in 45.38704061508179 seconds for 528 samples
Epoch: 551 	Training Loss: 0.022662 , computed in 44.79897665977478 seconds for 528 samples
Epoch: 552 	Training Loss: 0.022227 , computed in 45.44013595581055 seconds for 528 samples
Epoch: 553 	Training Loss: 0.023528 , computed in 44.953309535980225 seconds for 528 samples
Epoch: 554 	Training Loss: 0.022621 , computed in 45.06932497024536 seconds for 528 samples
INFO: Early stopping counter 9 of 1500 with -0.007002381607890129
Epoch: 555 	Training Loss: 0.023920 , computed in 45.46153116226196 seconds for 528 samples
Epoch: 556 	Training Loss: 0.021300 , computed in 45.250367164611816 seconds for 528 samples
Epoch: 557 	Training Loss: 0.022504 , computed in 45.27034378051758 seconds for 528 samples
Epoch: 558 	Training Loss: 0.021527 , computed in 45.013261556625366 seconds for 528 samples
Epoch: 559 	Training Loss: 0.021607 , computed in 45.26450490951538 seconds for 528 samples
INFO: Early stopping counter 10 of 1500 with -0.004857787862420082
Epoch: 560 	Training Loss: 0.021707 , computed in 45.57147002220154 seconds for 528 samples
Epoch: 561 	Training Loss: 0.023824 , computed in 44.951611280441284 seconds for 528 samples
Epoch: 562 	Training Loss: 0.022650 , computed in 45.38525414466858 seconds for 528 samples
Epoch: 563 	Training Loss: 0.023190 , computed in 45.33217716217041 seconds for 528 samples
Epoch: 564 	Training Loss: 0.022896 , computed in 45.38755965232849 seconds for 528 samples
INFO: Early stopping counter 11 of 1500 with -0.002145298756659031
Epoch: 565 	Training Loss: 0.023429 , computed in 45.53305673599243 seconds for 528 samples
Epoch: 566 	Training Loss: 0.022149 , computed in 45.260664224624634 seconds for 528 samples
Epoch: 567 	Training Loss: 0.024159 , computed in 44.97761845588684 seconds for 528 samples
Epoch: 568 	Training Loss: 0.022130 , computed in 45.59448862075806 seconds for 528 samples
Epoch: 569 	Training Loss: 0.022803 , computed in 45.855130195617676 seconds for 528 samples
INFO: Early stopping counter 12 of 1500 with -0.00029545649886131287
Epoch: 570 	Training Loss: 0.022100 , computed in 45.33048486709595 seconds for 528 samples
Epoch: 571 	Training Loss: 0.021763 , computed in 45.2077214717865 seconds for 528 samples
Epoch: 572 	Training Loss: 0.023865 , computed in 44.82631468772888 seconds for 528 samples
Epoch: 573 	Training Loss: 0.023905 , computed in 45.3144257068634 seconds for 528 samples
Epoch: 574 	Training Loss: 0.021835 , computed in 45.43413257598877 seconds for 528 samples
INFO: Early stopping counter 13 of 1500 with -0.0013401666656136513
Epoch: 575 	Training Loss: 0.021310 , computed in 45.43267560005188 seconds for 528 samples
Epoch: 576 	Training Loss: 0.023011 , computed in 45.016716957092285 seconds for 528 samples
Epoch: 577 	Training Loss: 0.022174 , computed in 44.93916416168213 seconds for 528 samples
Epoch: 578 	Training Loss: 0.022420 , computed in 44.70801115036011 seconds for 528 samples
Epoch: 579 	Training Loss: 0.023820 , computed in 45.094419956207275 seconds for 528 samples
INFO: Early stopping counter 14 of 1500 with -0.00048570893704891205
Epoch: 580 	Training Loss: 0.021117 , computed in 45.5417799949646 seconds for 528 samples
Epoch: 581 	Training Loss: 0.023898 , computed in 44.89203953742981 seconds for 528 samples
Epoch: 582 	Training Loss: 0.022413 , computed in 45.27798652648926 seconds for 528 samples
Epoch: 583 	Training Loss: 0.022156 , computed in 45.39353036880493 seconds for 528 samples
Epoch: 584 	Training Loss: 0.022949 , computed in 45.0637845993042 seconds for 528 samples
INFO: Early stopping counter 15 of 1500 with -0.0021655000746250153
Epoch: 585 	Training Loss: 0.022026 , computed in 45.236188650131226 seconds for 528 samples
Epoch: 586 	Training Loss: 0.021025 , computed in 45.18287205696106 seconds for 528 samples
Epoch: 587 	Training Loss: 0.022450 , computed in 45.16427659988403 seconds for 528 samples
Epoch: 588 	Training Loss: 0.023706 , computed in 45.23732542991638 seconds for 528 samples
Epoch: 589 	Training Loss: 0.022237 , computed in 45.32655048370361 seconds for 528 samples
INFO: Early stopping counter 16 of 1500 with -0.007956143468618393
Epoch: 590 	Training Loss: 0.022467 , computed in 45.39849400520325 seconds for 528 samples
Epoch: 591 	Training Loss: 0.022614 , computed in 44.962920904159546 seconds for 528 samples
Epoch: 592 	Training Loss: 0.023087 , computed in 45.43336844444275 seconds for 528 samples
Epoch: 593 	Training Loss: 0.023652 , computed in 45.20709991455078 seconds for 528 samples
Epoch: 594 	Training Loss: 0.022943 , computed in 44.72350454330444 seconds for 528 samples
Epoch: 595 	Training Loss: 0.024223 , computed in 45.37405300140381 seconds for 528 samples
Epoch: 596 	Training Loss: 0.022265 , computed in 45.44985389709473 seconds for 528 samples
Epoch: 597 	Training Loss: 0.022267 , computed in 45.39950084686279 seconds for 528 samples
Epoch: 598 	Training Loss: 0.022135 , computed in 45.07817053794861 seconds for 528 samples
Epoch: 599 	Training Loss: 0.023900 , computed in 45.01426339149475 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.0033993395045399666
Epoch: 600 	Training Loss: 0.021965 , computed in 45.31209325790405 seconds for 528 samples
Epoch: 601 	Training Loss: 0.022592 , computed in 43.887168169021606 seconds for 528 samples
Epoch: 602 	Training Loss: 0.022439 , computed in 43.74481534957886 seconds for 528 samples
Epoch: 603 	Training Loss: 0.022517 , computed in 43.60347318649292 seconds for 528 samples
Epoch: 604 	Training Loss: 0.022976 , computed in 43.13370180130005 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -4.8290006816387177e-05
Epoch: 605 	Training Loss: 0.024434 , computed in 45.98833179473877 seconds for 528 samples
Epoch: 606 	Training Loss: 0.022898 , computed in 46.03154921531677 seconds for 528 samples
Epoch: 607 	Training Loss: 0.023806 , computed in 49.99814414978027 seconds for 528 samples
Epoch: 608 	Training Loss: 0.022489 , computed in 48.95421838760376 seconds for 528 samples
Epoch: 609 	Training Loss: 0.023181 , computed in 48.87962770462036 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.002523251809179783
Epoch: 610 	Training Loss: 0.022961 , computed in 52.30651926994324 seconds for 528 samples
Epoch: 611 	Training Loss: 0.023282 , computed in 54.719383239746094 seconds for 528 samples
Epoch: 612 	Training Loss: 0.022237 , computed in 50.59662175178528 seconds for 528 samples
Epoch: 613 	Training Loss: 0.022570 , computed in 1777.3873658180237 seconds for 528 samples
Epoch: 614 	Training Loss: 0.023500 , computed in 42.45734906196594 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.002379787154495716
Epoch: 615 	Training Loss: 0.024531 , computed in 51.27893090248108 seconds for 528 samples
Epoch: 616 	Training Loss: 0.021824 , computed in 48.85549187660217 seconds for 528 samples
Epoch: 617 	Training Loss: 0.022845 , computed in 49.20018291473389 seconds for 528 samples
Epoch: 618 	Training Loss: 0.022523 , computed in 77.90287327766418 seconds for 528 samples
Epoch: 619 	Training Loss: 0.023586 , computed in 53.00300073623657 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0038029449060559273
Epoch: 620 	Training Loss: 0.022077 , computed in 77.803049325943 seconds for 528 samples
Epoch: 621 	Training Loss: 0.023975 , computed in 49.03913474082947 seconds for 528 samples
Epoch: 622 	Training Loss: 0.022529 , computed in 51.26215577125549 seconds for 528 samples
Epoch: 623 	Training Loss: 0.023713 , computed in 48.46603298187256 seconds for 528 samples
Epoch: 624 	Training Loss: 0.021919 , computed in 50.517735958099365 seconds for 528 samples
INFO: Early stopping counter 6 of 1500 with -0.002985558472573757
Epoch: 625 	Training Loss: 0.023868 , computed in 77.89301681518555 seconds for 528 samples
Epoch: 626 	Training Loss: 0.021316 , computed in 77.88693189620972 seconds for 528 samples
Epoch: 627 	Training Loss: 0.022240 , computed in 47.0973904132843 seconds for 528 samples
Epoch: 628 	Training Loss: 0.021727 , computed in 48.36563801765442 seconds for 528 samples
Epoch: 629 	Training Loss: 0.022428 , computed in 49.15936613082886 seconds for 528 samples
INFO: Early stopping counter 7 of 1500 with -0.0015189722180366516
Epoch: 630 	Training Loss: 0.022302 , computed in 107.93415832519531 seconds for 528 samples
Epoch: 631 	Training Loss: 0.021751 , computed in 51.77111768722534 seconds for 528 samples
Epoch: 632 	Training Loss: 0.025131 , computed in 48.987810373306274 seconds for 528 samples
Epoch: 633 	Training Loss: 0.021742 , computed in 51.50652098655701 seconds for 528 samples
Epoch: 634 	Training Loss: 0.022119 , computed in 47.44430065155029 seconds for 528 samples
INFO: Early stopping counter 8 of 1500 with -0.002898351289331913
Epoch: 635 	Training Loss: 0.022827 , computed in 53.858983278274536 seconds for 528 samples
Epoch: 636 	Training Loss: 0.023494 , computed in 2068.148848772049 seconds for 528 samples
Epoch: 637 	Training Loss: 0.022658 , computed in 46.78077220916748 seconds for 528 samples
Epoch: 638 	Training Loss: 0.021972 , computed in 48.865771532058716 seconds for 528 samples
Epoch: 639 	Training Loss: 0.020357 , computed in 59.603498458862305 seconds for 528 samples
INFO: Early stopping counter 9 of 1500 with -0.0030997833237051964
Epoch: 640 	Training Loss: 0.022889 , computed in 62.938199520111084 seconds for 528 samples
Epoch: 641 	Training Loss: 0.023523 , computed in 47.7752845287323 seconds for 528 samples
Epoch: 642 	Training Loss: 0.023764 , computed in 52.52783727645874 seconds for 528 samples
Epoch: 643 	Training Loss: 0.020478 , computed in 46.45964503288269 seconds for 528 samples
Epoch: 644 	Training Loss: 0.022296 , computed in 46.803696393966675 seconds for 528 samples
INFO: Early stopping counter 10 of 1500 with -0.005670475773513317
Epoch: 645 	Training Loss: 0.021393 , computed in 53.66341280937195 seconds for 528 samples
Epoch: 646 	Training Loss: 0.022251 , computed in 47.530022621154785 seconds for 528 samples
Epoch: 647 	Training Loss: 0.022206 , computed in 52.99131393432617 seconds for 528 samples
Epoch: 648 	Training Loss: 0.022317 , computed in 52.90258717536926 seconds for 528 samples
Epoch: 649 	Training Loss: 0.022194 , computed in 52.47126507759094 seconds for 528 samples
INFO: Early stopping counter 11 of 1500 with -0.009009911678731441
Epoch: 650 	Training Loss: 0.021210 , computed in 45.16407823562622 seconds for 528 samples
Epoch: 651 	Training Loss: 0.024128 , computed in 51.50034499168396 seconds for 528 samples
Epoch: 652 	Training Loss: 0.021737 , computed in 47.792064905166626 seconds for 528 samples
Epoch: 653 	Training Loss: 0.023722 , computed in 47.49654483795166 seconds for 528 samples
Epoch: 654 	Training Loss: 0.022412 , computed in 58.4530816078186 seconds for 528 samples
Epoch: 655 	Training Loss: 0.021677 , computed in 53.39476203918457 seconds for 528 samples
Epoch: 656 	Training Loss: 0.022399 , computed in 52.00772261619568 seconds for 528 samples
Epoch: 657 	Training Loss: 0.022591 , computed in 50.61723184585571 seconds for 528 samples
Epoch: 658 	Training Loss: 0.022957 , computed in 49.67996096611023 seconds for 528 samples
Epoch: 659 	Training Loss: 0.023771 , computed in 2639.376588344574 seconds for 528 samples
INFO: Early stopping counter 1 of 1500 with -0.015565157867968082
Epoch: 660 	Training Loss: 0.023026 , computed in 50.77732992172241 seconds for 528 samples
Epoch: 661 	Training Loss: 0.023820 , computed in 47.27080512046814 seconds for 528 samples
Epoch: 662 	Training Loss: 0.023845 , computed in 46.7527813911438 seconds for 528 samples
Epoch: 663 	Training Loss: 0.023130 , computed in 49.75999903678894 seconds for 528 samples
Epoch: 664 	Training Loss: 0.021556 , computed in 47.550013303756714 seconds for 528 samples
INFO: Early stopping counter 2 of 1500 with -0.006216668523848057
Epoch: 665 	Training Loss: 0.022524 , computed in 53.30005860328674 seconds for 528 samples
Epoch: 666 	Training Loss: 0.022177 , computed in 48.13916873931885 seconds for 528 samples
Epoch: 667 	Training Loss: 0.022317 , computed in 48.091789960861206 seconds for 528 samples
Epoch: 668 	Training Loss: 0.021213 , computed in 48.79895377159119 seconds for 528 samples
Epoch: 669 	Training Loss: 0.022482 , computed in 48.63518285751343 seconds for 528 samples
INFO: Early stopping counter 3 of 1500 with -0.002458672970533371
Epoch: 670 	Training Loss: 0.022697 , computed in 49.34830594062805 seconds for 528 samples
Epoch: 671 	Training Loss: 0.021579 , computed in 46.792109966278076 seconds for 528 samples
Epoch: 672 	Training Loss: 0.021636 , computed in 47.37256121635437 seconds for 528 samples
Epoch: 673 	Training Loss: 0.022815 , computed in 51.65194654464722 seconds for 528 samples
Epoch: 674 	Training Loss: 0.023832 , computed in 48.68913125991821 seconds for 528 samples
INFO: Early stopping counter 4 of 1500 with -0.0035878168419003487
Epoch: 675 	Training Loss: 0.023299 , computed in 45.30664014816284 seconds for 528 samples
Epoch: 676 	Training Loss: 0.022451 , computed in 59.259286403656006 seconds for 528 samples
Epoch: 677 	Training Loss: 0.022544 , computed in 48.65156602859497 seconds for 528 samples
Epoch: 678 	Training Loss: 0.021118 , computed in 48.725932359695435 seconds for 528 samples
Epoch: 679 	Training Loss: 0.023083 , computed in 52.73694658279419 seconds for 528 samples
INFO: Early stopping counter 5 of 1500 with -0.0025441227480769157
Exception in thread Thread-11:
Traceback (most recent call last):
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/wandb/filesync/step_upload.py", line 118, in _thread_body
    self._handle_event(event)
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/wandb/filesync/step_upload.py", line 180, in _handle_event
    self._start_upload_job(event)
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/wandb/filesync/step_upload.py", line 192, in _start_upload_job
    self._spawn_upload(event)
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/site-packages/wandb/filesync/step_upload.py", line 227, in _spawn_upload
    self._pool.submit(run_and_notify)
  File "/home/tanzl/miniconda3/envs/thor1/lib/python3.8/concurrent/futures/thread.py", line 179, in submit
    raise RuntimeError('cannot schedule new futures after shutdown')
RuntimeError: cannot schedule new futures after shutdown
